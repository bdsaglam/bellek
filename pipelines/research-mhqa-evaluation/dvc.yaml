stages:
  answer-questions:
    cmd: >-
      python answer_questions.py
      --dataset-file ../../data/raw/research-mhqa-evaluation/dataset.jsonl
      --out ../../data/generated/research-mhqa-evaluation/qa-results/
      --subset ${dataset.subset}
      --prompt ${qa.prompt}
      --model ${qa.model}
      --temperature ${qa.temperature}
    deps:
      - answer_questions.py
      - ../../data/raw/research-mhqa-evaluation/dataset.jsonl
    outs:
      - ../../data/generated/research-mhqa-evaluation/qa-results/
    params:
      - dataset.subset
      - qa.prompt
      - qa.model
      - qa.temperature
      - run
  
  evaluate-answers:
    cmd: >-
      python evaluate_answers.py
      --dataset-file ../../data/raw/research-mhqa-evaluation/dataset.jsonl
      --qa-dir ../../data/generated/research-mhqa-evaluation/qa-results/
      --out ../../data/generated/research-mhqa-evaluation/evals/
    deps:
      - evaluate_answers.py
      - ../../data/raw/research-mhqa-evaluation/dataset.jsonl
      - ../../data/generated/research-mhqa-evaluation/qa-results/
    outs:
      - ../../data/generated/research-mhqa-evaluation/evals/

  report:
    cmd: >-
      python report.py
      --dataset-file ../../data/raw/research-mhqa-evaluation/dataset.jsonl
      --qa-dir ../../data/generated/research-mhqa-evaluation/qa-results/
      --evals-dir ../../data/generated/research-mhqa-evaluation/evals/
      --out ../../data/generated/research-mhqa-evaluation/reports/
    deps:
      - report.py
      - ../../data/raw/research-mhqa-evaluation/dataset.jsonl
      - ../../data/generated/research-mhqa-evaluation/qa-results/
      - ../../data/generated/research-mhqa-evaluation/evals/
    outs:
      - ../../data/generated/research-mhqa-evaluation/reports/results.jsonl
    metrics:
      - ../../data/generated/research-mhqa-evaluation/reports/scores.json
