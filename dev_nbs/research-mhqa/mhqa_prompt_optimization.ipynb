{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Question Answering Pipeline\n",
    "\n",
    "This notebook implements a DSPy pipeline for optimizing question answering prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-70b-tgi\",\n",
    "    temperature=0.7,\n",
    "    cache=False,\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "     num_rows: 14376\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "     num_rows: 100\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_ds = load_dataset('bdsaglam/musique-2hop', 'answerable', split='train')\n",
    "val_ds = load_dataset('bdsaglam/musique-thesis', 'answerable', split='validation')\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '2hop__482757_12019',\n",
       " 'paragraphs': [{'idx': 0,\n",
       "   'title': 'Pakistan Super League',\n",
       "   'paragraph_text': 'Pakistan Super League (Urdu: پاکستان سپر لیگ \\u202c \\u200e; PSL) is a Twenty20 cricket league, founded in Lahore on 9 September 2015 with five teams and now comprises six teams. Instead of operating as an association of independently owned teams, the league is a single entity in which each franchise is owned and controlled by investors.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 1,\n",
       "   'title': 'Serena Wilson',\n",
       "   'paragraph_text': 'Serena Wilson (August 8, 1933 – June 17, 2007), often known just as \"Serena\", was a well-known dancer, choreographer, and teacher who helped popularize belly dance in the United States. Serena\\'s work also helped legitimize the dance form and helped it to be perceived as more than burlesque or stripping. Serena danced in clubs in her younger years, opened her own studio, hosted her own television show, founded her own dance troupe, and was the author of several books about belly dance.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 2,\n",
       "   'title': 'Longman',\n",
       "   'paragraph_text': 'Longman, also known as Pearson Longman, is a publishing company founded in London, England, in 1724 and is owned by Pearson PLC.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 3,\n",
       "   'title': 'Bankhaus Lampe',\n",
       "   'paragraph_text': 'Bankhaus Lampe is a private bank in Germany, founded in 1852 and headquartered in Bielefeld. It is wholly owned by the Oetker Group. The bank owns 50% of Universal Investment.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 4,\n",
       "   'title': 'Publix',\n",
       "   'paragraph_text': 'Publix Super Markets, Inc., commonly known as Publix, is an employee - owned, American supermarket chain headquartered in Lakeland, Florida. Founded in 1930 by George W. Jenkins, Publix is a private corporation that is wholly owned by present and past employees. It is considered the largest employee - owned company in the world. Publix operates throughout the Southeastern United States, with locations in Florida (785), Georgia (186), Alabama (68), South Carolina (58), Tennessee (42), North Carolina (35), and Virginia (8).',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 5,\n",
       "   'title': 'The Collegian (Houston Baptist University)',\n",
       "   'paragraph_text': 'The Collegian is the bi-weekly official student publication of Houston Baptist University in Houston, Texas. It was founded in 1963 as a newsletter, and adopted the newspaper format in 1990.',\n",
       "   'is_supporting': True},\n",
       "  {'idx': 6,\n",
       "   'title': 'The Collegian (Hillsdale College)',\n",
       "   'paragraph_text': \"The Collegian is the oldest college newspaper in Michigan. The paper's history traces back to 1878, when the Hillsdale Herald was first published. The administration started The Collegian in 1893 as a rival paper to the Herald.\",\n",
       "   'is_supporting': False},\n",
       "  {'idx': 7,\n",
       "   'title': 'List of Old Scotch Collegians',\n",
       "   'paragraph_text': 'This is a list of Old Scotch Collegians, who are notable former students of Scotch College in Melbourne, Victoria, Australia.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 8,\n",
       "   'title': 'Renaissance Broadcasting',\n",
       "   'paragraph_text': 'Renaissance Broadcasting, founded in 1982 by Michael Finkelstein, was a company that owned several UHF television stations, it was sold to Tribune Broadcasting in 1997. The company was headquartered in Greenwich, Connecticut.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 9,\n",
       "   'title': 'Houston',\n",
       "   'paragraph_text': \"Several private institutions of higher learning—ranging from liberal arts colleges, such as The University of St. Thomas, Houston's only Catholic university, to Rice University, the nationally recognized research university—are located within the city. Rice, with a total enrollment of slightly more than 6,000 students, has a number of distinguished graduate programs and research institutes, such as the James A. Baker Institute for Public Policy. Houston Baptist University, affiliated with the Baptist General Convention of Texas, offers bachelor's and graduate degrees. It was founded in 1960 and is located in the Sharpstown area in Southwest Houston.\",\n",
       "   'is_supporting': True},\n",
       "  {'idx': 10,\n",
       "   'title': 'France-Guyane',\n",
       "   'paragraph_text': 'France-Guyane is a daily, French-language newspaper headquartered in Cayenne, French Guiana. Founded in 1973, the newspaper is owned by \"French-Antilles\", which is controlled by the Groupe Hersant Média group.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 11,\n",
       "   'title': 'Bayou City Broadcasting',\n",
       "   'paragraph_text': 'Bayou City Broadcasting, LLC is a broadcasting company founded in December 2007 and owned by DuJuan McCoy. The company is based in The Woodlands, Texas.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 12,\n",
       "   'title': 'GKS Górnik 1979 Łęczna',\n",
       "   'paragraph_text': 'GKS Górnik 1979 Łęczna was a short-lived fan-owned phoenix club founded in 2011 by Górnik Łęczna fans who were unhappy with the name change to GKS Bogdanka. The club eventually changed its name back in 2013 but the fan owned counterpart has continued to operate in amateur football leagues until 2014.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 13,\n",
       "   'title': 'Broadway Federal Bank',\n",
       "   'paragraph_text': 'The Broadway Federal Bank is a community bank founded in 1946 and based in Los Angeles. As of 2011, it owned and operated three traditional branches and one loan production office.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 14,\n",
       "   'title': 'Gubernija',\n",
       "   'paragraph_text': 'Gubernija is a brewery in Lithuania. It is one of the oldest businesses in the world, having been founded in 1665. Gubernija is listed on the NASDAQ OMX Vilnius stock exchange. Unlike other Lithuanian breweries, Gubernija has its own pubs.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 15,\n",
       "   'title': 'Nordic Paper',\n",
       "   'paragraph_text': 'Nordic Paper AS is a Norwegian industrial company, operating in Norway and Sweden. It was founded in 2001 when \"Peterson Scanproof\", a branch of M. Peterson & Søn which consisted of production units in Greåker (formerly owned by Greaker Industrier) and Säffle, was merged with a paper factory in Geithus, owned by Norske Skog Union.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 16,\n",
       "   'title': 'Logica',\n",
       "   'paragraph_text': 'Logica was a multinational IT and management consultancy company headquartered in Reading, United Kingdom. Founded in 1969, the company became a wholly owned subsidiary of CGI Group in 2012.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 17,\n",
       "   'title': 'Skipton Business Finance',\n",
       "   'paragraph_text': 'Skipton Business Finance is a UK factoring and Invoice discounting company, founded and based in Skipton, North Yorkshire. It is a wholly owned subsidiary of Skipton Building Society.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 18,\n",
       "   'title': 'Brett Kearney',\n",
       "   'paragraph_text': 'Brett Kearney (born 29 September 1983 in Sydney, New South Wales), also known by the nickname of \"BK\", is an Australian professional rugby league footballer formerly with the Bradford Bulls in the Super League, now currently playing for the Collegians in the Illawarra Rugby League competition. A utility back, he has represented Country Origin and previously played for the South Sydney Rabbitohs and Cronulla.',\n",
       "   'is_supporting': False},\n",
       "  {'idx': 19,\n",
       "   'title': 'Grand Duchy of Baden State Railway',\n",
       "   'paragraph_text': 'The Grand Duchy of Baden was an independent state in what is now southwestern Germany until the creation of the German Empire in 1871. It had its own state-owned railway company, the Grand Duchy of Baden State Railways (\"Großherzoglich Badische Staatseisenbahnen or G.Bad.St.E.\"), which was founded in 1840. At the time when it was integrated into the Deutsche Reichsbahn in 1920, its network had an overall length of about .',\n",
       "   'is_supporting': False}],\n",
       " 'question': 'When was the institute that owned The Collegian founded?',\n",
       " 'question_decomposition': [{'id': 482757,\n",
       "   'question': 'The Collegian >> owned by',\n",
       "   'answer': 'Houston Baptist University',\n",
       "   'paragraph_support_idx': 5},\n",
       "  {'id': 12019,\n",
       "   'question': 'When was #1 founded?',\n",
       "   'answer': '1960',\n",
       "   'paragraph_support_idx': 9}],\n",
       " 'answer': '1960',\n",
       " 'answer_aliases': [],\n",
       " 'answerable': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(record):\n",
    "    id2paragraph = {p['idx']: p for p in record['paragraphs']}\n",
    "    supporting_paragraphs = [id2paragraph[qd['paragraph_support_idx']]['paragraph_text'] for qd in record['question_decomposition']]\n",
    "    return dspy.Example(\n",
    "        question=record['question'],\n",
    "        paragraphs=supporting_paragraphs,\n",
    "        answers=[record['answer'], *record['answer_aliases']],\n",
    "    ).with_inputs('question', 'paragraphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When was the institute that owned The Collegian founded?',\n",
       " 'paragraphs': ['The Collegian is the bi-weekly official student publication of Houston Baptist University in Houston, Texas. It was founded in 1963 as a newsletter, and adopted the newspaper format in 1990.',\n",
       "  \"Several private institutions of higher learning—ranging from liberal arts colleges, such as The University of St. Thomas, Houston's only Catholic university, to Rice University, the nationally recognized research university—are located within the city. Rice, with a total enrollment of slightly more than 6,000 students, has a number of distinguished graduate programs and research institutes, such as the James A. Baker Institute for Public Policy. Houston Baptist University, affiliated with the Baptist General Convention of Texas, offers bachelor's and graduate degrees. It was founded in 1960 and is located in the Sharpstown area in Southwest Houston.\"],\n",
       " 'answers': ['1960']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = [make_example(record) for record in train_ds][:100]\n",
    "valset = [make_example(record) for record in val_ds]\n",
    "dict(trainset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposeQuestion(dspy.Signature):\n",
    "    \"\"\"Decompose a complex question into simpler (usually 2, 3 or 4) sub-questions . Example:\n",
    "Where did the player who scored the most points in a NBA season go in the NBA Draft?\n",
    "1) Who has the most points in a NBA season?\n",
    "2) Where did #1 go in the NBA draft?\n",
    "\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField()\n",
    "    decomposition: str = dspy.OutputField(\n",
    "        desc=\"Enumerated list of sub-questions, using '#n >>' notation for dependent questions\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subquestions(decomposition_str):\n",
    "    for line in decomposition_str.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            parts = line.split(\") \", 1)\n",
    "            if len(parts) == 1:\n",
    "                yield parts[0].strip()\n",
    "            elif len(parts) == 2:\n",
    "                yield parts[1].strip()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid decomposition line: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who has the most points in a NBA season?',\n",
       " 'Where did #1 go in the NBA draft?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(split_subquestions(\"\"\"\n",
    "1) Who has the most points in a NBA season?\n",
    "2) Where did #1 go in the NBA draft?\n",
    "\"\"\".strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer question based on the given context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"The context to use for answering the question.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"The factual answer to the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qdecomp = dspy.Predict(DecomposeQuestion)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question, paragraphs):\n",
    "        # Decompose the question into sub-questions\n",
    "        sub_questions = list(split_subquestions(self.qdecomp(question=question).decomposition))\n",
    "        dspy.Suggest(len(sub_questions) == len(paragraphs), f\"This question is composed of {len(paragraphs)} sub-questions, but you generated {len(sub_questions)} sub-questions.\")\n",
    "\n",
    "        # Answer each sub-question in sequence  \n",
    "        sub_answers = []\n",
    "        for i, (paragraph, sub_q) in enumerate(zip(paragraphs, sub_questions)):\n",
    "            if i == 0:\n",
    "                pred = self.generate_answer(context=paragraph, question=sub_q)\n",
    "                sub_answers.append(pred.answer)\n",
    "            else:\n",
    "                sub_q = sub_q.replace(f\"#{i}\", sub_answers[i-1])\n",
    "                pred = self.generate_answer(context=paragraph, question=sub_q)\n",
    "                sub_answers.append(pred.answer)\n",
    "\n",
    "        return dspy.Prediction(answer=sub_answers[-1], sub_questions=sub_questions, sub_answers=sub_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the uncompiled QA module\n",
    "uncompiled_qa = SimplifiedBaleen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the optimization metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import answer_exact_match_str\n",
    "\n",
    "def evaluate_answer(example, pred, trace=None):\n",
    "    return answer_exact_match_str(pred.answer, example.answers, frac=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_qa = Evaluate(devset=valset, metric=evaluate_answer, num_threads=8, display_progress=True, return_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0): 100%|██████████| 100/100 [03:56<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Question Answering Score: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score, uncompiled_results = evaluate_qa(uncompiled_qa)\n",
    "print(f\"Uncompiled Question Answering Score: {uncompiled_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21 / 49  (42.9):  49%|████▉     | 49/100 [02:15<01:33,  1.84s/it]\u001b[2m2024-10-09T19:03:13.639809Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 4 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 48.0 / 100  (48.0): 100%|██████████| 100/100 [04:02<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 48.0 for seed -3\n",
      "Scores so far: [48.0]\n",
      "Best score so far: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22 / 47  (46.8):  47%|████▋     | 47/100 [01:43<02:22,  2.68s/it]\u001b[2m2024-10-09T19:06:46.779294Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 51.0 / 100  (51.0): 100%|██████████| 100/100 [03:34<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 51.0 for seed -2\n",
      "Scores so far: [48.0, 51.0]\n",
      "Best score so far: 51.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [02:14<12:40,  8.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 16 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27 / 49  (55.1):  49%|████▉     | 49/100 [03:26<03:16,  3.85s/it]\u001b[2m2024-10-09T19:14:16.354612Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 52.0 / 96  (54.2):  96%|█████████▌| 96/100 [06:33<00:10,  2.63s/it]\u001b[2m2024-10-09T19:17:24.454317Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 54.0 / 100  (54.0): 100%|██████████| 100/100 [06:48<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 54.0 for seed -1\n",
      "Scores so far: [48.0, 51.0, 54.0]\n",
      "Best score so far: 54.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [02:34<14:42, 10.03s/it]\u001b[2m2024-10-09T19:20:15.194335Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to run or to evaluate example Example({'question': 'What is the literacy rate in the main city near where Guerra was assassinated?', 'paragraphs': ['But the peace in the state did not last long, the elections of 1875 caused new hostilities. Ángel Trías led a new movement against the government in June 1875 and maintained control over the government until September 18, 1875 when Donato Guerra the orchestrator of the Revolution of the North was captured. Donato Guerra was assassinated in a suburb of Chihuahua City where he was incarcerated for conspiring with Ángel Trías. During October 1875 several locations were controlled by rebel forces, but the government finally regained control on November 25, 1875.', 'The state has one city with a population exceeding one million: Ciudad Juárez. Ciudad Juárez is ranked eighth most populous city in the country and Chihuahua City was ranked 16th most populous in Mexico. Chihuahua (along with Baja California) is the only state in Mexico to have two cities ranked in the top 20 most populated. El Paso and Ciudad Juárez comprise one of the largest binational metropolitan areas in the world with a combined population of 2.4 million. In fact, Ciudad Juárez is one of the fastest growing cities in the world in spite of the fact that it is \"the most violent zone in the world outside of declared war zones\". For instance, a few years ago the Federal Reserve Bank of Dallas published that in Ciudad Juárez \"the average annual growth over the 10-year period 1990–2000 was 5.3 percent. Juárez experienced much higher population growth than the state of Chihuahua and than Mexico as a whole\". Chihuahua City has one of the highest literacy rates in the country at 98%; 35% of the population is aged 14 or below, 60% 15-65, and 5% over 65. The growth rate is 2.4%. The 76.5% of the population of the state of Chihuahua live in cities which makes the state one of the most urbanized in Mexico.'], 'answers': ['98%']}) (input_keys={'paragraphs', 'question'}) with <function evaluate_answer at 0x70a1093896c0> due to This question is composed of 2 sub-questions, but only 3 sub-questions were generated..\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.teleprompt.bootstrap\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbootstrap.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m211\u001b[0m\n",
      " 15%|█▌        | 15/100 [02:56<16:41, 11.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 16 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 48  (54.2):  48%|████▊     | 48/100 [03:13<02:55,  3.38s/it]\u001b[2m2024-10-09T19:23:53.630471Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 54.0 / 100  (54.0): 100%|██████████| 100/100 [06:56<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0]\n",
      "Best score so far: 54.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [05:23<2:54:20, 107.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17 / 28  (60.7):  28%|██▊       | 28/100 [01:35<03:26,  2.87s/it]\u001b[2m2024-10-09T19:34:31.992928Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 55.0 / 100  (55.0): 100%|██████████| 100/100 [04:56<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 55.0 for seed 1\n",
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0]\n",
      "Best score so far: 55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:05<09:32,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15 / 28  (53.6):  28%|██▊       | 28/100 [00:59<02:00,  1.67s/it]\u001b[2m2024-10-09T19:38:57.702205Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 55.0 / 100  (55.0): 100%|██████████| 100/100 [03:28<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0]\n",
      "Best score so far: 55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [03:40<37:12, 24.54s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5 / 6  (83.3):   6%|▌         | 6/100 [01:52<15:42, 10.02s/it]   \u001b[2m2024-10-09T19:47:34.218870Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 55.0 / 100  (55.0): 100%|██████████| 100/100 [08:59<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0]\n",
      "Best score so far: 55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [08:00<1:04:48, 43.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 49  (53.1):  49%|████▉     | 49/100 [02:25<01:53,  2.23s/it]\u001b[2m2024-10-09T20:04:34.723420Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 54.0 / 100  (54.0): 100%|██████████| 100/100 [04:53<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0]\n",
      "Best score so far: 55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [01:48<14:41,  9.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 29 / 48  (60.4):  48%|████▊     | 48/100 [02:53<03:11,  3.69s/it]\u001b[2m2024-10-09T20:11:45.570165Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 59.0 / 100  (59.0): 100%|██████████| 100/100 [05:52<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 59.0 for seed 5\n",
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0, 59.0]\n",
      "Best score so far: 59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:49<15:37,  9.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 50 / 100  (50.0): 100%|██████████| 100/100 [32:20<00:00, 19.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0, 59.0, 50.0]\n",
      "Best score so far: 59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [46:37<5:12:01, 215.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 14 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14 / 28  (50.0):  28%|██▊       | 28/100 [01:42<03:19,  2.77s/it]\u001b[2m2024-10-09T21:36:16.324726Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 25.0 / 48  (52.1):  48%|████▊     | 48/100 [02:54<02:51,  3.31s/it]\u001b[2m2024-10-09T21:37:26.722864Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 56.0 / 100  (56.0): 100%|██████████| 100/100 [05:48<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0, 59.0, 50.0, 56.0]\n",
      "Best score so far: 59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [01:17<13:06,  8.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5 / 6  (83.3):   6%|▌         | 6/100 [00:20<03:33,  2.27s/it] \u001b[2m2024-10-09T21:42:02.037658Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 21.0 / 47  (44.7):  47%|████▋     | 47/100 [02:13<02:15,  2.56s/it]\u001b[2m2024-10-09T21:43:55.650013Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 52.0 / 100  (52.0): 100%|██████████| 100/100 [04:35<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0, 59.0, 50.0, 56.0, 52.0]\n",
      "Best score so far: 59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [01:40<13:29,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5 / 6  (83.3):   6%|▌         | 6/100 [00:34<07:34,  4.83s/it] \u001b[2m2024-10-09T21:48:34.294404Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 13.0 / 28  (46.4):  28%|██▊       | 28/100 [01:56<03:27,  2.88s/it]\u001b[2m2024-10-09T21:49:55.797493Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 45.0 / 84  (53.6):  84%|████████▍ | 84/100 [05:50<00:51,  3.21s/it]\u001b[2m2024-10-09T21:53:50.637741Z\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mReceived SIGINT. Cancelling evaluation.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m105\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot, LabeledFewShot, BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Set up the teleprompter\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "    metric=evaluate_answer, \n",
    "    max_bootstrapped_demos=8, \n",
    ")\n",
    "\n",
    "# Compile and optimize the QA module\n",
    "compiled_qa = teleprompter.compile(uncompiled_qa, trainset=trainset)\n",
    "compiled_qa.save('compiled-qa.json')\n",
    "\n",
    "print(\"QA module compiled and optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_qa.save('compiled-qa.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 10  (10.0): 100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n",
      "Uncompiled QA Module Score: 10.0\n",
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:12<00:00,  1.25s/it]\n",
      "Compiled QA Module Score: 30.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the compiled QA module\n",
    "compiled_score, compiled_results = evaluate_qa(compiled_qa)\n",
    "print(f\"Compiled QA Module Score: {compiled_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Optional) Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_errors(results):\n",
    "    errors = [(example, pred) for example, pred, score in results if score < 1.0] \n",
    "    for example, pred in errors:\n",
    "        print(f\"Question: {example.question}\")\n",
    "        print(f\"Context: {example.context}\")\n",
    "        print(f\"Groundtruth Answers: {example.answers}\")\n",
    "        print(f\"Predicted Answer: {pred.answer}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error analysis for uncompiled program:\")\n",
    "present_errors(uncompiled_results)\n",
    "\n",
    "print(\"Error analysis for compiled program:\")\n",
    "present_errors(compiled_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
