{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Question Answering Pipeline\n",
    "\n",
    "This notebook implements a DSPy pipeline for optimizing question answering prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-70b-tgi\",\n",
    "    temperature=0.7,\n",
    "    cache=False,\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "     num_rows: 14376\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "     num_rows: 100\n",
       " }))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_ds = load_dataset('bdsaglam/musique-2hop', 'answerable', split='train')\n",
    "val_ds = load_dataset('bdsaglam/musique-thesis', 'answerable', split='validation')\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(record):\n",
    "    id2paragraph = {p['idx']: p for p in record['paragraphs']}\n",
    "    supporting_paragraphs = [id2paragraph[qd['paragraph_support_idx']]['paragraph_text'] for qd in record['question_decomposition']]\n",
    "    return dspy.Example(\n",
    "        question=record['question'],\n",
    "        paragraphs=supporting_paragraphs,\n",
    "        answers=[record['answer'], *record['answer_aliases']],\n",
    "    ).with_inputs('question', 'paragraphs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When was the institute that owned The Collegian founded?',\n",
       " 'paragraphs': ['The Collegian is the bi-weekly official student publication of Houston Baptist University in Houston, Texas. It was founded in 1963 as a newsletter, and adopted the newspaper format in 1990.',\n",
       "  \"Several private institutions of higher learning—ranging from liberal arts colleges, such as The University of St. Thomas, Houston's only Catholic university, to Rice University, the nationally recognized research university—are located within the city. Rice, with a total enrollment of slightly more than 6,000 students, has a number of distinguished graduate programs and research institutes, such as the James A. Baker Institute for Public Policy. Houston Baptist University, affiliated with the Baptist General Convention of Texas, offers bachelor's and graduate degrees. It was founded in 1960 and is located in the Sharpstown area in Southwest Houston.\"],\n",
       " 'answers': ['1960']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = [make_example(record) for record in train_ds][:100]\n",
    "valset = [make_example(record) for record in val_ds]\n",
    "dict(trainset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposeQuestion(dspy.Signature):\n",
    "    \"\"\"Decompose a complex question into simpler (usually 2, 3 or 4) sub-questions . Example:\n",
    "Where did the player who scored the most points in a NBA season go in the NBA Draft?\n",
    "1) Who has the most points in a NBA season?\n",
    "2) Where did #1 go in the NBA draft?\n",
    "\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField()\n",
    "    decomposition: str = dspy.OutputField(\n",
    "        desc=\"Enumerated list of sub-questions, using '#n >>' notation for dependent questions\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_subquestions(decomposition_str):\n",
    "    for line in decomposition_str.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            parts = line.split(\") \", 1)\n",
    "            if len(parts) == 1:\n",
    "                yield parts[0].strip()\n",
    "            elif len(parts) == 2:\n",
    "                yield parts[1].strip()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid decomposition line: {line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who has the most points in a NBA season?',\n",
       " 'Where did #1 go in the NBA draft?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(split_subquestions(\"\"\"\n",
    "1) Who has the most points in a NBA season?\n",
    "2) Where did #1 go in the NBA draft?\n",
    "\"\"\".strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer question based on the given context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"The context to use for answering the question.\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"The factual answer to the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qdecomp = dspy.Predict(DecomposeQuestion)\n",
    "        self.generate_answer = dspy.ChainOfThought(GenerateAnswer)\n",
    "    \n",
    "    def forward(self, question, paragraphs):\n",
    "        # Decompose the question into sub-questions\n",
    "        sub_questions = list(split_subquestions(self.qdecomp(question=question).decomposition))\n",
    "        dspy.Suggest(len(sub_questions) == len(paragraphs), f\"This question is composed of {len(paragraphs)} sub-questions, but you generated {len(sub_questions)} sub-questions.\")\n",
    "\n",
    "        # Answer each sub-question in sequence  \n",
    "        sub_answers = []\n",
    "        for i, (paragraph, sub_q) in enumerate(zip(paragraphs, sub_questions)):\n",
    "            if i == 0:\n",
    "                pred = self.generate_answer(context=paragraph, question=sub_q)\n",
    "                sub_answers.append(pred.answer)\n",
    "            else:\n",
    "                sub_q = sub_q.replace(f\"#{i}\", sub_answers[i-1])\n",
    "                pred = self.generate_answer(context=paragraph, question=sub_q)\n",
    "                sub_answers.append(pred.answer)\n",
    "\n",
    "        return dspy.Prediction(answer=sub_answers[-1], sub_questions=sub_questions, sub_answers=sub_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the uncompiled QA module\n",
    "uncompiled_qa = SimplifiedBaleen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the optimization metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import answer_exact_match_str\n",
    "\n",
    "def evaluate_answer(example, pred, trace=None):\n",
    "    return answer_exact_match_str(pred.answer, example.answers, frac=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_qa = Evaluate(devset=valset, metric=evaluate_answer, num_threads=8, display_progress=True, return_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 60 / 100  (60.0): 100%|██████████| 100/100 [03:56<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Question Answering Score: 60.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score, uncompiled_results = evaluate_qa(uncompiled_qa)\n",
    "print(f\"Uncompiled Question Answering Score: {uncompiled_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 21 / 49  (42.9):  49%|████▉     | 49/100 [02:15<01:33,  1.84s/it]\u001b[2m2024-10-09T19:03:13.639809Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 4 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 48.0 / 100  (48.0): 100%|██████████| 100/100 [04:02<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 48.0 for seed -3\n",
      "Scores so far: [48.0]\n",
      "Best score so far: 48.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 22 / 47  (46.8):  47%|████▋     | 47/100 [01:43<02:22,  2.68s/it]\u001b[2m2024-10-09T19:06:46.779294Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 51.0 / 100  (51.0): 100%|██████████| 100/100 [03:34<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 51.0 for seed -2\n",
      "Scores so far: [48.0, 51.0]\n",
      "Best score so far: 51.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [02:14<12:40,  8.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 16 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27 / 49  (55.1):  49%|████▉     | 49/100 [03:26<03:16,  3.85s/it]\u001b[2m2024-10-09T19:14:16.354612Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 52.0 / 96  (54.2):  96%|█████████▌| 96/100 [06:33<00:10,  2.63s/it]\u001b[2m2024-10-09T19:17:24.454317Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 54.0 / 100  (54.0): 100%|██████████| 100/100 [06:48<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 54.0 for seed -1\n",
      "Scores so far: [48.0, 51.0, 54.0]\n",
      "Best score so far: 54.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [02:34<14:42, 10.03s/it]\u001b[2m2024-10-09T19:20:15.194335Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mFailed to run or to evaluate example Example({'question': 'What is the literacy rate in the main city near where Guerra was assassinated?', 'paragraphs': ['But the peace in the state did not last long, the elections of 1875 caused new hostilities. Ángel Trías led a new movement against the government in June 1875 and maintained control over the government until September 18, 1875 when Donato Guerra the orchestrator of the Revolution of the North was captured. Donato Guerra was assassinated in a suburb of Chihuahua City where he was incarcerated for conspiring with Ángel Trías. During October 1875 several locations were controlled by rebel forces, but the government finally regained control on November 25, 1875.', 'The state has one city with a population exceeding one million: Ciudad Juárez. Ciudad Juárez is ranked eighth most populous city in the country and Chihuahua City was ranked 16th most populous in Mexico. Chihuahua (along with Baja California) is the only state in Mexico to have two cities ranked in the top 20 most populated. El Paso and Ciudad Juárez comprise one of the largest binational metropolitan areas in the world with a combined population of 2.4 million. In fact, Ciudad Juárez is one of the fastest growing cities in the world in spite of the fact that it is \"the most violent zone in the world outside of declared war zones\". For instance, a few years ago the Federal Reserve Bank of Dallas published that in Ciudad Juárez \"the average annual growth over the 10-year period 1990–2000 was 5.3 percent. Juárez experienced much higher population growth than the state of Chihuahua and than Mexico as a whole\". Chihuahua City has one of the highest literacy rates in the country at 98%; 35% of the population is aged 14 or below, 60% 15-65, and 5% over 65. The growth rate is 2.4%. The 76.5% of the population of the state of Chihuahua live in cities which makes the state one of the most urbanized in Mexico.'], 'answers': ['98%']}) (input_keys={'paragraphs', 'question'}) with <function evaluate_answer at 0x70a1093896c0> due to This question is composed of 2 sub-questions, but only 3 sub-questions were generated..\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.teleprompt.bootstrap\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mbootstrap.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m211\u001b[0m\n",
      " 15%|█▌        | 15/100 [02:56<16:41, 11.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 16 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 48  (54.2):  48%|████▊     | 48/100 [03:13<02:55,  3.38s/it]\u001b[2m2024-10-09T19:23:53.630471Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 54.0 / 100  (54.0): 100%|██████████| 100/100 [06:56<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0]\n",
      "Best score so far: 54.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [05:23<2:54:20, 107.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 17 / 28  (60.7):  28%|██▊       | 28/100 [01:35<03:26,  2.87s/it]\u001b[2m2024-10-09T19:34:31.992928Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 55.0 / 100  (55.0): 100%|██████████| 100/100 [04:56<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 55.0 for seed 1\n",
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0]\n",
      "Best score so far: 55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:05<09:32,  5.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 15 / 28  (53.6):  28%|██▊       | 28/100 [00:59<02:00,  1.67s/it]\u001b[2m2024-10-09T19:38:57.702205Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 55.0 / 100  (55.0): 100%|██████████| 100/100 [03:28<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0]\n",
      "Best score so far: 55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [03:40<37:12, 24.54s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5 / 6  (83.3):   6%|▌         | 6/100 [01:52<15:42, 10.02s/it]   \u001b[2m2024-10-09T19:47:34.218870Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 55.0 / 100  (55.0): 100%|██████████| 100/100 [08:59<00:00,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0]\n",
      "Best score so far: 55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [08:00<1:04:48, 43.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 49  (53.1):  49%|████▉     | 49/100 [02:25<01:53,  2.23s/it]\u001b[2m2024-10-09T20:04:34.723420Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 54.0 / 100  (54.0): 100%|██████████| 100/100 [04:53<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0]\n",
      "Best score so far: 55.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [01:48<14:41,  9.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 29 / 48  (60.4):  48%|████▊     | 48/100 [02:53<03:11,  3.69s/it]\u001b[2m2024-10-09T20:11:45.570165Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 59.0 / 100  (59.0): 100%|██████████| 100/100 [05:52<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 59.0 for seed 5\n",
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0, 59.0]\n",
      "Best score so far: 59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:49<15:37,  9.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 50 / 100  (50.0): 100%|██████████| 100/100 [32:20<00:00, 19.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0, 59.0, 50.0]\n",
      "Best score so far: 59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [46:37<5:12:01, 215.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 14 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 14 / 28  (50.0):  28%|██▊       | 28/100 [01:42<03:19,  2.77s/it]\u001b[2m2024-10-09T21:36:16.324726Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 25.0 / 48  (52.1):  48%|████▊     | 48/100 [02:54<02:51,  3.31s/it]\u001b[2m2024-10-09T21:37:26.722864Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 56.0 / 100  (56.0): 100%|██████████| 100/100 [05:48<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0, 59.0, 50.0, 56.0]\n",
      "Best score so far: 59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [01:17<13:06,  8.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5 / 6  (83.3):   6%|▌         | 6/100 [00:20<03:33,  2.27s/it] \u001b[2m2024-10-09T21:42:02.037658Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 21.0 / 47  (44.7):  47%|████▋     | 47/100 [02:13<02:15,  2.56s/it]\u001b[2m2024-10-09T21:43:55.650013Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 52.0 / 100  (52.0): 100%|██████████| 100/100 [04:35<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [48.0, 51.0, 54.0, 54.0, 55.0, 55.0, 55.0, 54.0, 59.0, 50.0, 56.0, 52.0]\n",
      "Best score so far: 59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [01:40<13:29,  9.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 5 / 6  (83.3):   6%|▌         | 6/100 [00:34<07:34,  4.83s/it] \u001b[2m2024-10-09T21:48:34.294404Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 13.0 / 28  (46.4):  28%|██▊       | 28/100 [01:56<03:27,  2.88s/it]\u001b[2m2024-10-09T21:49:55.797493Z\u001b[0m [\u001b[31m\u001b[1merror    \u001b[0m] \u001b[1mError for example in dev set: \t\t This question is composed of 2 sub-questions, but only 3 sub-questions were generated.. Set `provide_traceback=True` to see the stack trace.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m198\u001b[0m\n",
      "Average Metric: 45.0 / 84  (53.6):  84%|████████▍ | 84/100 [05:50<00:51,  3.21s/it]\u001b[2m2024-10-09T21:53:50.637741Z\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mReceived SIGINT. Cancelling evaluation.\u001b[0m [\u001b[0m\u001b[1m\u001b[34mdspy.evaluate.evaluate\u001b[0m]\u001b[0m \u001b[36mfilename\u001b[0m=\u001b[35mevaluate.py\u001b[0m \u001b[36mlineno\u001b[0m=\u001b[35m105\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m teleprompter \u001b[38;5;241m=\u001b[39m BootstrapFewShotWithRandomSearch(\n\u001b[1;32m      5\u001b[0m     metric\u001b[38;5;241m=\u001b[39mevaluate_answer, \n\u001b[1;32m      6\u001b[0m     max_bootstrapped_demos\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, \n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Compile and optimize the QA module\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m compiled_qa \u001b[38;5;241m=\u001b[39m \u001b[43mteleprompter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43muncompiled_qa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m compiled_qa\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompiled-qa.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQA module compiled and optimized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/baris/lib/python3.10/site-packages/dspy/teleprompt/random_search.py:117\u001b[0m, in \u001b[0;36mBootstrapFewShotWithRandomSearch.compile\u001b[0;34m(self, student, teacher, trainset, valset, restrict, labeled_sample)\u001b[0m\n\u001b[1;32m    106\u001b[0m     program \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mcompile(student, teacher\u001b[38;5;241m=\u001b[39mteacher, trainset\u001b[38;5;241m=\u001b[39mtrainset_copy)\n\u001b[1;32m    108\u001b[0m evaluate \u001b[38;5;241m=\u001b[39m Evaluate(\n\u001b[1;32m    109\u001b[0m     devset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalset,\n\u001b[1;32m    110\u001b[0m     metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     display_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    115\u001b[0m )\n\u001b[0;32m--> 117\u001b[0m score, subscores \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_all_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m all_subscores\u001b[38;5;241m.\u001b[39mappend(subscores)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m############ Assertion-aware Optimization ############\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/baris/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:211\u001b[0m, in \u001b[0;36mEvaluate.__call__\u001b[0;34m(self, program, metric, devset, num_threads, display_progress, display_table, return_all_scores, return_outputs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_single_thread(wrapped_program, devset, display_progress)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 211\u001b[0m     reordered_devset, ncorrect, ntotal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_multi_thread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwrapped_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Metric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mncorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mntotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39mncorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mntotal,\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    220\u001b[0m predicted_devset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(reordered_devset)\n",
      "File \u001b[0;32m~/anaconda3/envs/baris/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:123\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread\u001b[0;34m(self, wrapped_program, devset, num_threads, display_progress)\u001b[0m\n\u001b[1;32m    120\u001b[0m futures \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(cancellable_wrapped_program, idx, arg) \u001b[38;5;28;01mfor\u001b[39;00m idx, arg \u001b[38;5;129;01min\u001b[39;00m devset}\n\u001b[1;32m    121\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(devset), dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m display_progress)\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[1;32m    124\u001b[0m     example_idx, example, prediction, score \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# use the cancelled_job literal to check if the job was cancelled - use \"is\" not \"==\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;66;03m# in case the prediction is \"cancelled\" for some reason.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/baris/lib/python3.10/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wait_timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    248\u001b[0m     finished \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39mfinished_futures\n",
      "File \u001b[0;32m~/anaconda3/envs/baris/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/baris/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/baris/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:106\u001b[0m, in \u001b[0;36mEvaluate._execute_multi_thread.<locals>.interrupt_handler_manager.<locals>.interrupt_handler\u001b[0;34m(sig, frame)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_jobs\u001b[38;5;241m.\u001b[39mset()\n\u001b[1;32m    105\u001b[0m dspy\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived SIGINT. Cancelling evaluation.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m \u001b[43mdefault_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot, LabeledFewShot, BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Set up the teleprompter\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "    metric=evaluate_answer, \n",
    "    max_bootstrapped_demos=8, \n",
    ")\n",
    "\n",
    "# Compile and optimize the QA module\n",
    "compiled_qa = teleprompter.compile(uncompiled_qa, trainset=trainset)\n",
    "compiled_qa.save('compiled-qa.json')\n",
    "\n",
    "print(\"QA module compiled and optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_qa.save('compiled-qa.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 1 / 10  (10.0): 100%|██████████| 10/10 [00:12<00:00,  1.21s/it]\n",
      "Uncompiled QA Module Score: 10.0\n",
      "Average Metric: 3 / 10  (30.0): 100%|██████████| 10/10 [00:12<00:00,  1.25s/it]\n",
      "Compiled QA Module Score: 30.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the compiled QA module\n",
    "compiled_score, compiled_results = evaluate_qa(compiled_qa)\n",
    "print(f\"Compiled QA Module Score: {compiled_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Optional) Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_errors(results):\n",
    "    errors = [(example, pred) for example, pred, score in results if score < 1.0] \n",
    "    for example, pred in errors:\n",
    "        print(f\"Question: {example.question}\")\n",
    "        print(f\"Context: {example.context}\")\n",
    "        print(f\"Groundtruth Answers: {example.answers}\")\n",
    "        print(f\"Predicted Answer: {pred.answer}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error analysis for uncompiled program:\")\n",
    "present_errors(uncompiled_results)\n",
    "\n",
    "print(\"Error analysis for compiled program:\")\n",
    "present_errors(compiled_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
