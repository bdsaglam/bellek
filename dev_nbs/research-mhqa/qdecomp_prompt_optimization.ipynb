{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-70b-tgi\",\n",
    "    temperature=0.7,\n",
    "    cache=False,\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "        num_rows: 19938\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "        num_rows: 2417\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dsd = load_dataset('bdsaglam/musique', 'answerable')\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that samples from the dataset with equal distribution of n_hops\n",
    "def sample_evenly(dataset, n_samples):\n",
    "    dataset = dataset.map(lambda x: {'n_hops': len(x['question_decomposition'])})\n",
    "    n_hops = np.unique(dataset['n_hops'])\n",
    "    samples_per_hop = n_samples // len(n_hops)\n",
    "    for hop in n_hops:\n",
    "        hop_samples = dataset.filter(lambda x: x['n_hops'] == hop).shuffle().select(range(samples_per_hop))\n",
    "        yield from hop_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = list(sample_evenly(dsd['train'], 30))\n",
    "val_samples = list(sample_evenly(dsd['validation'], 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(record):\n",
    "    decomposition = '\\n'.join([f\"{i+1}. {item['question']}\" for i, item in enumerate(record[\"question_decomposition\"])])\n",
    "    return dspy.Example(\n",
    "        question=record[\"question\"],\n",
    "        decomposition=decomposition,\n",
    "    ).with_inputs(\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'Why did the Battle of the state where Me, Myself and Irene takes place happen?', 'decomposition': '1. where did me myself and irene take place\\n2. why did the battle of #1 happen'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_example(train_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [make_example(record) for record in train_samples]\n",
    "valset = [make_example(record) for record in val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Why did the Battle of the state where Me, Myself and Irene takes place happen?',\n",
       " 'decomposition': '1. where did me myself and irene take place\\n2. why did the battle of #1 happen'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(example):\n",
    "    print(example.question)\n",
    "    print(example.decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What social activities have Muslim's contributed to in The Mystic Masseur film's producer's birth country?\n",
      "1. The Mystic Masseur >> producer\n",
      "2. #1 >> place of birth\n",
      "3. in which country is the city of #2\n",
      "4. What are the roles of Muslims across #3 ?\n",
      "\n",
      "Who failed to start an English colony off the coast of the state that borders the east of the state where Hello Love's performer lived in when he died?\n",
      "1. Hello Love >> performer\n",
      "2. What city did #1 live when he died?\n",
      "3. Which state borders #2 to the east?\n",
      "4. who failed in his attempt to start an english colony off the coast of #3\n",
      "\n",
      "The county where Tiffany Scott was born shares a border with what county?\n",
      "1. Tiffany Scott >> place of birth\n",
      "2. #1 >> located in the administrative territorial entity\n",
      "3. #2 >> shares border with\n",
      "\n",
      "In which episode of Doctor Who did a character based on the creator of Trees and Undergrowth appear?\n",
      "1. Trees and Undergrowth >> creator\n",
      "2. episode of doctor who with #1\n",
      "\n",
      "What is the highest point where the Green-breasted pitta can be found in the country where Kitgum is located?\n",
      "1. Kitgum >> country\n",
      "2. What is the highest point which it can be found in #1 ?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in random.sample(trainset,5):\n",
    "    print_example(example)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "## Exact Match\n",
    "def split_subquestions(decomposition_str):\n",
    "    for line in decomposition_str.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            parts = line.split(\". \", 1)\n",
    "            if len(parts) == 1:\n",
    "                return parts[0].strip\n",
    "            elif len(parts) == 2:\n",
    "                yield parts[1].strip()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid decomposition line: {line}\")\n",
    "\n",
    "\n",
    "# Update the evaluation function\n",
    "def evaluate_decomposition_exact_match(example, pred, trace=None):\n",
    "    gold_sub_questions = list(split_subquestions(example.decomposition))\n",
    "    pred_sub_questions = list(split_subquestions(pred.decomposition))\n",
    "\n",
    "    assert len(gold_sub_questions), \"Gold decomposition is empty.\"\n",
    "\n",
    "    exact_matches = len([1 for gold, pred in zip(gold_sub_questions, pred_sub_questions) if gold == pred])\n",
    "    accuracy = exact_matches / len(gold_sub_questions)\n",
    "    return accuracy\n",
    "\n",
    "## LLM as Judge\n",
    "class DecompositionJudge(dspy.Signature):\n",
    "    \"\"\"Judge whether the predicted decomposition matches the ground truth.\n",
    "\n",
    "    Instructions:\n",
    "    - Given a ground-truth decomposition and a predicted decomposition, assess whether they are equivalent in meaning.\n",
    "    - Consider whether the steps correspond logically, even if worded differently.\n",
    "    - Output 'Yes' if they are equivalent, 'No' otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth: str = dspy.InputField(desc=\"The ground-truth decomposition\")\n",
    "    prediction: str = dspy.InputField(desc=\"The predicted decomposition\")\n",
    "    equivalent: str = dspy.OutputField(desc=\"Are the decompositions equivalent? [Yes/No]\", prefix=\"Equivalent[Yes/No]:\")\n",
    "\n",
    "qdecomp_judge = dspy.Predict(DecompositionJudge)\n",
    "\n",
    "# Updated evaluation function using the judge\n",
    "def evaluate_decomposition_llm(example, pred, trace=None):\n",
    "    result = qdecomp_judge(\n",
    "        ground_truth=example.decomposition,\n",
    "        prediction=pred.decomposition,\n",
    "    )\n",
    "    is_equivalent = result.equivalent.strip().lower()\n",
    "    return int(is_equivalent == \"yes\")\n",
    "\n",
    "\n",
    "## Combined\n",
    "\n",
    "def evaluate_decomposition(example, pred, trace=None):\n",
    "    accuracy = evaluate_decomposition_exact_match(example, pred, trace)\n",
    "    if accuracy >= 0.8:\n",
    "        return accuracy\n",
    "    return evaluate_decomposition_llm(example, pred, trace)\n",
    "\n",
    "\n",
    "# Set up the evaluation function\n",
    "evaluate_qd = Evaluate(devset=valset, metric=evaluate_decomposition, num_threads=8, display_progress=True, return_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_errors(results):\n",
    "    errors = [(example, pred) for example, pred, score in results if score < 1.0] \n",
    "    for example, pred in errors:\n",
    "        print(f\"Original Question: {example.question}\")\n",
    "        print(f\"# Groundtruth Decomposition\\n{example.decomposition}\")\n",
    "        print(f\"# Predicted Decomposition\\n{pred.decomposition}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposeQuestion(dspy.Signature):\n",
    "    \"\"\"Decompose a complex question into simpler sub-questions.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField()\n",
    "    decomposition: str = dspy.OutputField(\n",
    "        desc=\"Enumerated list of sub-questions, using '#n >>' notation for dependent questions\"\n",
    "    )\n",
    "\n",
    "class QuestionDecompositionModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decompose = dspy.Predict(DecomposeQuestion)\n",
    "\n",
    "    def forward(self, question):\n",
    "        pred = self.decompose(question=question)\n",
    "        return dspy.Prediction(decomposition=pred.decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [00:36<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Question Decomposition Score: 86.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the uncompiled question decomposition module\n",
    "uncompiled_qd = QuestionDecompositionModule()\n",
    "\n",
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score, uncompiled_results = evaluate_qd(uncompiled_qd, return_outputs=True)\n",
    "print(f\"Uncompiled Question Decomposition Score: {uncompiled_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 25 / 30  (83.3): 100%|██████████| 30/30 [00:41<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 83.33 for seed -3\n",
      "Scores so far: [83.33]\n",
      "Best score so far: 83.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [00:51<00:00,  1.73s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 86.67 for seed -2\n",
      "Scores so far: [83.33, 86.67]\n",
      "Best score so far: 86.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:37<01:43,  4.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27 / 30  (90.0): 100%|██████████| 30/30 [00:46<00:00,  1.54s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 90.0 for seed -1\n",
      "Scores so far: [83.33, 86.67, 90.0]\n",
      "Best score so far: 90.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [01:03<01:35,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 13 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24 / 30  (80.0): 100%|██████████| 30/30 [00:49<00:00,  1.65s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0]\n",
      "Best score so far: 90.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:23<02:33,  5.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 24 / 30  (80.0): 100%|██████████| 30/30 [00:49<00:00,  1.66s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0]\n",
      "Best score so far: 90.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:04<02:13,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [00:48<00:00,  1.62s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67]\n",
      "Best score so far: 90.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:22<02:25,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 28 / 30  (93.3): 100%|██████████| 30/30 [00:53<00:00,  1.80s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 93.33 for seed 3\n",
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:28<02:23,  5.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27 / 30  (90.0): 100%|██████████| 30/30 [00:49<00:00,  1.66s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:30<02:01,  5.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27 / 30  (90.0): 100%|██████████| 30/30 [00:50<00:00,  1.67s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:25<02:48,  6.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [00:51<00:00,  1.73s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:38<02:07,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [00:48<00:00,  1.61s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67, 86.67]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:43<02:21,  6.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [00:50<00:00,  1.69s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67, 86.67, 86.67]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:41<01:23,  4.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 11 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 25 / 30  (83.3): 100%|██████████| 30/30 [01:10<00:00,  2.34s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67, 86.67, 86.67, 83.33]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:20<09:59, 20.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [02:55<00:00,  5.85s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67, 86.67, 86.67, 83.33, 86.67]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [03:58<07:57, 23.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 11 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [05:07<00:00, 10.25s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67, 86.67, 86.67, 83.33, 86.67, 86.67]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [11:19<16:59, 56.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 13 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [06:30<00:00, 13.03s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67, 86.67, 86.67, 83.33, 86.67, 86.67, 86.67]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [08:03<32:15, 80.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [07:36<00:00, 15.21s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67, 86.67, 86.67, 83.33, 86.67, 86.67, 86.67, 86.67]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [02:42<37:57, 81.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 25 / 30  (83.3): 100%|██████████| 30/30 [07:30<00:00, 15.01s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67, 86.67, 86.67, 83.33, 86.67, 86.67, 86.67, 86.67, 83.33]\n",
      "Best score so far: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [06:48<34:00, 81.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 27 / 30  (90.0): 100%|██████████| 30/30 [07:14<00:00, 14.48s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [83.33, 86.67, 90.0, 80.0, 80.0, 86.67, 93.33, 90.0, 90.0, 86.67, 86.67, 86.67, 83.33, 86.67, 86.67, 86.67, 86.67, 83.33, 90.0]\n",
      "Best score so far: 93.33\n",
      "19 candidate programs found.\n",
      "[('decompose', Predict(DecomposeQuestion(question -> decomposition\n",
      "    instructions='Decompose a complex question into simpler sub-questions.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    decomposition = Field(annotation=str required=True json_schema_extra={'desc': \"Enumerated list of sub-questions, using '#n >>' notation for dependent questions\", '__dspy_field_type': 'output', 'prefix': 'Decomposition:'})\n",
      ")))]\n",
      "Question Decomposition module compiled and optimized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot, LabeledFewShot, BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Set up the teleprompter\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "    metric=evaluate_decomposition, \n",
    "    max_bootstrapped_demos=8, \n",
    "    max_labeled_demos=8,\n",
    ")\n",
    "\n",
    "# Compile and optimize the question decomposition module\n",
    "compiled_qd = teleprompter.compile(uncompiled_qd, trainset=trainset, valset=valset)\n",
    "compiled_qd.save('qdecomp-program-compiled.json')\n",
    "print(\"Question Decomposition module compiled and optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 28 / 30  (93.3): 100%|██████████| 30/30 [05:59<00:00, 11.99s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled Question Decomposition Score: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the compiled question decomposition module\n",
    "compiled_score, compiled_results = evaluate_qd(compiled_qd, return_outputs=True)\n",
    "print(f\"Compiled Question Decomposition Score: {compiled_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analysis for uncompiled question decomposition:\n",
      "Original Question: The developer of Mozilla Sunbird created which browser?\n",
      "# Groundtruth Decomposition\n",
      "1. Mozilla Sunbird >> developer\n",
      "2. What was the resulting browser for #1 ?\n",
      "# Predicted Decomposition\n",
      "#1 >> What is Mozilla Sunbird?\n",
      "#2 >> Who developed Mozilla Sunbird?\n",
      "#3 >> What browser did the developer of Mozilla Sunbird create?\n",
      "\n",
      "Original Question: Where did the player who scored the most points in a NBA season go in the NBA Draft?\n",
      "# Groundtruth Decomposition\n",
      "1. who has the most points in a nba season\n",
      "2. where did #1 go in the nba draft\n",
      "# Predicted Decomposition\n",
      "#1 >> Who scored the most points in a NBA season?\n",
      "#2 >> In what year did this player score the most points?\n",
      "#3 >> Where did this player go in the NBA Draft?\n",
      "\n",
      "Original Question: What is the meaning of the word that is also a majority religion in what became India when the country that disavowed the Taliban was created in the Arabic dictionary?\n",
      "# Groundtruth Decomposition\n",
      "1. Which country disavowed the Taliban?\n",
      "2. What was the majority religion in the area of British India that become India when #1 was created?\n",
      "3. what is the meaning of #2 in arabic dictionary\n",
      "# Predicted Decomposition\n",
      "#1 >> What is the country that disavowed the Taliban?\n",
      "#2 >> When was the country that disavowed the Taliban created?\n",
      "#3 >> What was India before the country that disavowed the Taliban was created?\n",
      "#4 >> What is the majority religion in what became India?\n",
      "#5 >> What is the word that is also a majority religion in what became India?\n",
      "#6 >> What is the meaning of the word in the Arabic dictionary?\n",
      "\n",
      "Original Question: What county contains the city with a radio station that broadcasts to the capital city of the state where the Peace center is located?\n",
      "# Groundtruth Decomposition\n",
      "1. Which state is Peace Center located?\n",
      "2. What city became the state capital of #1 ?\n",
      "3. #2 >> shares border with\n",
      "4. #3 >> located in the administrative territorial entity\n",
      "# Predicted Decomposition\n",
      "#1 >> What state is the Peace center located in?\n",
      "#2 >> What is the capital city of that state?\n",
      "#3 >> What city has a radio station that broadcasts to that capital city?\n",
      "#4 >> What county contains that city?\n",
      "\n",
      "Error analysis for compiled question decomposition:\n",
      "Original Question: When was the Palau de la Generalitat constructed in the city where Martin from the region where Codo is located died?\n",
      "# Groundtruth Decomposition\n",
      "1. Codo >> located in the administrative territorial entity\n",
      "2. Martin of #1 >> place of death\n",
      "3. When was the Palau de la Generalitat in #2 constructed?\n",
      "# Predicted Decomposition\n",
      "1. Codo >> region\n",
      "2. Martin >> place of death\n",
      "3. What city is #2 in?\n",
      "4. When was the Palau de la Generalitat constructed in #3?\n",
      "\n",
      "Original Question: What county contains the city with a radio station that broadcasts to the capital city of the state where the Peace center is located?\n",
      "# Groundtruth Decomposition\n",
      "1. Which state is Peace Center located?\n",
      "2. What city became the state capital of #1 ?\n",
      "3. #2 >> shares border with\n",
      "4. #3 >> located in the administrative territorial entity\n",
      "# Predicted Decomposition\n",
      "1. Where is the Peace Center located?\n",
      "2. What is the capital city of #1's state?\n",
      "3. What city has a radio station that broadcasts to #2?\n",
      "4. What county contains #3?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Error analysis for uncompiled question decomposition:\")\n",
    "present_errors(uncompiled_results)\n",
    "\n",
    "print(\"Error analysis for compiled question decomposition:\")\n",
    "present_errors(compiled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
