{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-70b-tgi\",\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "        num_rows: 19938\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "        num_rows: 2417\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dsd = load_dataset('bdsaglam/musique', 'answerable')\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(record):\n",
    "    decomposition = '\\n'.join([f\"{i+1}. {item['question']}\" for i, item in enumerate(record[\"question_decomposition\"])])\n",
    "    return dspy.Example(\n",
    "        question=record[\"question\"],\n",
    "        decomposition=decomposition,\n",
    "    ).with_inputs(\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'When was the institute that owned The Collegian founded?', 'decomposition': '1. The Collegian >> owned by\\n2. When was #1 founded?'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_example(dsd['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "trainset = [make_example(record) for record in islice(iter(dsd['train']), 100)]\n",
    "evalset = [make_example(record) for record in islice(iter(dsd['validation']), 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'When was the institute that owned The Collegian founded?',\n",
       " 'decomposition': '1. The Collegian >> owned by\\n2. When was #1 founded?'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(example):\n",
    "    print(f\"Question: {example.question}\")\n",
    "    print(\"# Decomposition\")\n",
    "    print(example.decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: When did the country formerly known as Zaire become independent?\n",
      "# Decomposition\n",
      "1. zaire is the former name of which african nation\n",
      "2. when did #1 become independent\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What is the record label for the person who sang Beauty and the Beast with Celine Dion?\n",
      "# Decomposition\n",
      "1. who did celine dion sing beauty and the beast with\n",
      "2. #1 >> record label\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What year was the end of the company that built the White armored car?\n",
      "# Decomposition\n",
      "1. What company built White armored car?\n",
      "2. What year was the end of #1 ?\n",
      "--------------------------------------------------------------------------------\n",
      "Question: When did the institute that gives out the Elliott Cresson Medal award open?\n",
      "# Decomposition\n",
      "1. Who gives out the Elliott Cresson Medal award?\n",
      "2. When did #1 open?\n",
      "--------------------------------------------------------------------------------\n",
      "Question: What is the field of work of the proposer of the modern synthetic theory of evolution?\n",
      "# Decomposition\n",
      "1. who proposed the modern synthetic theory of evolution\n",
      "2. #1 >> field of work\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for example in random.sample(trainset,5):\n",
    "    print_example(example)\n",
    "    print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecompositionJudge(dspy.Signature):\n",
    "    \"\"\"Judge whether the predicted decomposition matches the ground truth.\n",
    "\n",
    "    Instructions:\n",
    "    - Given a ground-truth decomposition and a predicted decomposition, assess whether they are equivalent in meaning.\n",
    "    - Consider whether the steps correspond logically, even if worded differently.\n",
    "    - Output 'Yes' if they are equivalent, 'No' otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth: str = dspy.InputField(desc=\"The ground-truth decomposition\")\n",
    "    prediction: str = dspy.InputField(desc=\"The predicted decomposition\")\n",
    "    equivalent: str = dspy.OutputField(desc=\"Are the decompositions equivalent? [Yes/No]\", prefix=\"Equivalent[Yes/No]:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the Signature\n",
    "class DecomposeQuestion(dspy.Signature):\n",
    "    \"\"\"Decompose a complex question into simpler sub-questions.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField()\n",
    "    decomposition: str = dspy.OutputField(\n",
    "        desc=\"Enumerated list of sub-questions, using '#n >>' notation for dependent questions\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Update the Module\n",
    "class QuestionDecompositionModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decompose = dspy.ChainOfThought(DecomposeQuestion)\n",
    "\n",
    "    def forward(self, question):\n",
    "        pred = self.decompose(question=question)\n",
    "        return dspy.Prediction(decomposition=pred.decomposition)\n",
    "\n",
    "\n",
    "def split_subquestions(decomposition_str):\n",
    "    for line in decomposition_str.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            parts = line.split(\". \", 1)\n",
    "            if len(parts) == 1:\n",
    "                return parts[0].strip\n",
    "            elif len(parts) == 2:\n",
    "                yield parts[1].strip()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid decomposition line: {line}\")\n",
    "\n",
    "\n",
    "# Update the evaluation function\n",
    "def evaluate_decomposition_exact_match(example, pred, trace=None):\n",
    "    gold_sub_questions = list(split_subquestions(example.decomposition))\n",
    "    pred_sub_questions = list(split_subquestions(pred.decomposition))\n",
    "\n",
    "    assert len(gold_sub_questions), \"Gold decomposition is empty.\"\n",
    "\n",
    "    exact_matches = len([1 for gold, pred in zip(gold_sub_questions, pred_sub_questions) if gold == pred])\n",
    "    accuracy = exact_matches / len(gold_sub_questions)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "qdecomp_judge = dspy.Predict(DecompositionJudge)\n",
    "\n",
    "\n",
    "# Updated evaluation function using the judge\n",
    "def evaluate_decomposition_llm(example, pred, trace=None):\n",
    "    result = qdecomp_judge(\n",
    "        ground_truth=example.decomposition,\n",
    "        prediction=pred.decomposition,\n",
    "    )\n",
    "    is_equivalent = result.equivalent.strip().lower()\n",
    "    return int(is_equivalent == \"yes\")\n",
    "\n",
    "\n",
    "def evaluate_decomposition(example, pred, trace=None):\n",
    "    accuracy = evaluate_decomposition_exact_match(example, pred, trace)\n",
    "    if accuracy >= 0.8:\n",
    "        return accuracy\n",
    "    return evaluate_decomposition_llm(example, pred, trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Decomposition module compiled and optimized\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot, LabeledFewShot, BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# ## 6. Implement the optimization process\n",
    "# Initialize the uncompiled question decomposition module\n",
    "uncompiled_qd = QuestionDecompositionModule()\n",
    "\n",
    "# Set up the teleprompter\n",
    "teleprompter = LabeledFewShot(k=8)\n",
    "\n",
    "# Compile and optimize the question decomposition module\n",
    "compiled_qd = teleprompter.compile(uncompiled_qd, trainset=trainset)\n",
    "\n",
    "print(\"Question Decomposition module compiled and optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 2 / 2  (100.0):   2%|▏         | 2/100 [01:28<1:13:31, 45.01s/it]"
     ]
    }
   ],
   "source": [
    "# Set up the evaluation function\n",
    "evaluate_qd = Evaluate(devset=evalset, metric=evaluate_decomposition, num_threads=1, display_progress=True)\n",
    "\n",
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score = evaluate_qd(uncompiled_qd)\n",
    "print(f\"Uncompiled Question Decomposition Module Score: {uncompiled_score}\")\n",
    "\n",
    "# Evaluate the compiled question decomposition module\n",
    "compiled_score = evaluate_qd(compiled_qd)\n",
    "print(f\"Compiled Question Decomposition Module Score: {compiled_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 8. (Optional) Error Analysis\n",
    "\n",
    "\n",
    "def perform_error_analysis(qd_module, dataset, num_samples=5):\n",
    "    errors = []\n",
    "    for example in dataset:\n",
    "        pred = qd_module(question=example.question)\n",
    "        if not evaluate_decomposition(example, pred):\n",
    "            errors.append((example, pred))\n",
    "    \n",
    "    print(f\"Total errors: {len(errors)}\")\n",
    "    print(\"\\nSample of errors:\")\n",
    "    for example, pred in errors[:num_samples]:\n",
    "        print(f\"Original Question: {example.question}\")\n",
    "        print(f\"# True Decomposition\\n{example.decomposition}\")\n",
    "        print(f\"# Predicted Decomposition\\n{pred.decomposition}\")\n",
    "        print()\n",
    "\n",
    "print(\"Error analysis for uncompiled Question Decomposition module:\")\n",
    "perform_error_analysis(uncompiled_qd, evalset)\n",
    "\n",
    "print(\"\\nError analysis for compiled Question Decomposition module:\")\n",
    "perform_error_analysis(compiled_qd, evalset)\n",
    "\n",
    "\n",
    "compiled_qd.predictors()[0]\n",
    "\n",
    "\n",
    "compiled_qd.save('compiled-qd.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
