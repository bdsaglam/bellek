{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-70b-tgi\",\n",
    "    temperature=0.7,\n",
    "    cache=False,\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "        num_rows: 19938\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "        num_rows: 2417\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dsd = load_dataset('bdsaglam/musique', 'answerable')\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that samples from the dataset with equal distribution of n_hops\n",
    "def sample_evenly(dataset, n_samples):\n",
    "    dataset = dataset.map(lambda x: {'n_hops': len(x['question_decomposition'])})\n",
    "    n_hops = np.unique(dataset['n_hops'])\n",
    "    samples_per_hop = n_samples // len(n_hops)\n",
    "    for hop in n_hops:\n",
    "        hop_samples = dataset.filter(lambda x: x['n_hops'] == hop).shuffle().select(range(samples_per_hop))\n",
    "        yield from hop_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = list(sample_evenly(dsd['train'], 30))\n",
    "val_samples = list(sample_evenly(dsd['validation'], 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(record):\n",
    "    decomposition = '\\n'.join([f\"{i+1}. {item['question']}\" for i, item in enumerate(record[\"question_decomposition\"])])\n",
    "    return dspy.Example(\n",
    "        question=record[\"question\"],\n",
    "        decomposition=decomposition,\n",
    "    ).with_inputs(\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': 'Who was the ninth governor-general in the country where Victory is located?', 'decomposition': '1. Victory >> country\\n2. Who was the ninth governor-general in #1 ?'}) (input_keys={'question'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_example(train_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [make_example(record) for record in train_samples]\n",
    "valset = [make_example(record) for record in val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Who was the ninth governor-general in the country where Victory is located?',\n",
       " 'decomposition': '1. Victory >> country\\n2. Who was the ninth governor-general in #1 ?'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(example):\n",
    "    print(example.question)\n",
    "    print(example.decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was manager when Aziz Deen-Conteh's team won the Champions League?\n",
      "1. Aziz Deen-Conteh >> member of sports team\n",
      "2. who was manager when #1 won champions league\n",
      "\n",
      "What do they call the local government of the city where Spectre filming moved after the city where the author of Cum nimis absurdum died?\n",
      "1. Cum nimis absurdum >> author\n",
      "2. #1 >> place of death\n",
      "3. Where did Spectre filming take place after #2 ?\n",
      "4. What is the local government of #3 called?\n",
      "\n",
      "There is a group of islands among which is one that received COM status in 2007 alongside St Barts. When did the people who received support from Posen in the Franco-Prussian War come to those islands?\n",
      "1. What island besides St. Barts was granted COM status by France in 2007?\n",
      "2. #1 (French part) >> located on terrain feature\n",
      "3. What was there strong support of in Posen?\n",
      "4. when did the #3 come to the #2\n",
      "\n",
      "1994 Tour of the country whose official name is sometimes known as the country having Boesingheliede is a type of what?\n",
      "1. Boesingheliede >> country\n",
      "2. what is the official name of the country sometimes known as #1\n",
      "3. 1994 Tour of the #2 >> instance of\n",
      "\n",
      "Who founded the university attended by Jonathan Russell?\n",
      "1. Jonathan Russell >> educated at\n",
      "2. #1 >> founded by\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in random.sample(trainset,5):\n",
    "    print_example(example)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "## Exact Match\n",
    "def split_subquestions(decomposition_str):\n",
    "    for line in decomposition_str.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            parts = line.split(\". \", 1)\n",
    "            if len(parts) == 1:\n",
    "                return parts[0].strip\n",
    "            elif len(parts) == 2:\n",
    "                yield parts[1].strip()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid decomposition line: {line}\")\n",
    "\n",
    "\n",
    "# Update the evaluation function\n",
    "def evaluate_decomposition_exact_match(example, pred, trace=None):\n",
    "    gold_sub_questions = list(split_subquestions(example.decomposition))\n",
    "    pred_sub_questions = list(split_subquestions(pred.decomposition))\n",
    "\n",
    "    assert len(gold_sub_questions), \"Gold decomposition is empty.\"\n",
    "\n",
    "    exact_matches = len([1 for gold, pred in zip(gold_sub_questions, pred_sub_questions) if gold == pred])\n",
    "    accuracy = exact_matches / len(gold_sub_questions)\n",
    "    return accuracy\n",
    "\n",
    "## LLM as Judge\n",
    "class DecompositionJudge(dspy.Signature):\n",
    "    \"\"\"Judge whether the predicted decomposition matches the ground truth.\n",
    "\n",
    "    Instructions:\n",
    "    - Given a ground-truth decomposition and a predicted decomposition, assess whether they are equivalent in meaning.\n",
    "    - Consider whether the steps correspond logically, even if worded differently.\n",
    "    - Output 'Yes' if they are equivalent, 'No' otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth: str = dspy.InputField(desc=\"The ground-truth decomposition\")\n",
    "    prediction: str = dspy.InputField(desc=\"The predicted decomposition\")\n",
    "    equivalent: str = dspy.OutputField(desc=\"Are the decompositions equivalent? [Yes/No]\", prefix=\"Equivalent[Yes/No]:\")\n",
    "\n",
    "qdecomp_judge = dspy.Predict(DecompositionJudge)\n",
    "\n",
    "# Updated evaluation function using the judge\n",
    "def evaluate_decomposition_llm(example, pred, trace=None):\n",
    "    result = qdecomp_judge(\n",
    "        ground_truth=example.decomposition,\n",
    "        prediction=pred.decomposition,\n",
    "    )\n",
    "    is_equivalent = result.equivalent.strip().lower()\n",
    "    return int(is_equivalent == \"yes\")\n",
    "\n",
    "\n",
    "## Combined\n",
    "\n",
    "def evaluate_decomposition(example, pred, trace=None):\n",
    "    accuracy = evaluate_decomposition_exact_match(example, pred, trace)\n",
    "    if accuracy >= 0.8:\n",
    "        return accuracy\n",
    "    return evaluate_decomposition_llm(example, pred, trace)\n",
    "\n",
    "\n",
    "# Set up the evaluation function\n",
    "evaluate_qd = Evaluate(devset=valset, metric=evaluate_decomposition, num_threads=8, display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_errors(results):\n",
    "    errors = [(example, pred) for example, pred, score in results if score < 1.0] \n",
    "    for example, pred in errors:\n",
    "        print(f\"Original Question: {example.question}\")\n",
    "        print(f\"# Groundtruth Decomposition\\n{example.decomposition}\")\n",
    "        print(f\"# Predicted Decomposition\\n{pred.decomposition}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposeQuestion(dspy.Signature):\n",
    "    \"\"\"Decompose a complex question into simpler sub-questions.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField()\n",
    "    decomposition: str = dspy.OutputField(\n",
    "        desc=\"Enumerated list of sub-questions, using '#n >>' notation for dependent questions\"\n",
    "    )\n",
    "\n",
    "class QuestionDecompositionModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decompose = dspy.Predict(DecomposeQuestion)\n",
    "\n",
    "    def forward(self, question):\n",
    "        pred = self.decompose(question=question)\n",
    "        return dspy.Prediction(decomposition=pred.decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 25 / 30  (83.3): 100%|██████████| 30/30 [00:28<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Question Decomposition Score: 83.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the uncompiled question decomposition module\n",
    "uncompiled_qd = QuestionDecompositionModule()\n",
    "\n",
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score, uncompiled_results = evaluate_qd(uncompiled_qd, return_outputs=True)\n",
    "print(f\"Uncompiled Question Decomposition Score: {uncompiled_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23 / 30  (76.7): 100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 76.67 for seed -3\n",
      "Scores so far: [76.67]\n",
      "Best score so far: 76.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 23 / 30  (76.7): 100%|██████████| 30/30 [00:40<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [76.67, 76.67]\n",
      "Best score so far: 76.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:24<01:08,  3.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 26 / 30  (86.7): 100%|██████████| 30/30 [00:36<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 86.67 for seed -1\n",
      "Scores so far: [76.67, 76.67, 86.67]\n",
      "Best score so far: 86.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:27<01:16,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 9 / 12  (75.0):  37%|███▋      | 11/30 [00:16<00:18,  1.05it/s]"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot, LabeledFewShot, BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Set up the teleprompter\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "    metric=evaluate_decomposition, \n",
    "    max_bootstrapped_demos=8, \n",
    "    max_labeled_demos=8,\n",
    ")\n",
    "\n",
    "# Compile and optimize the question decomposition module\n",
    "compiled_qd = teleprompter.compile(uncompiled_qd, trainset=trainset, valset=valset)\n",
    "compiled_qd.save('qdecomp-program-compiled.json')\n",
    "print(\"Question Decomposition module compiled and optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 8 / 8  (100.0):  27%|██▋       | 8/30 [00:49<01:05,  2.99s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 28 / 30  (93.3): 100%|██████████| 30/30 [03:06<00:00,  6.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled Question Decomposition Score: 93.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the compiled question decomposition module\n",
    "compiled_score, compiled_results = evaluate_qd(compiled_qd, return_outputs=True)\n",
    "print(f\"Compiled Question Decomposition Score: {compiled_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analysis for uncompiled question decomposition:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'uncompiled_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError analysis for uncompiled question decomposition:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m present_errors(\u001b[43muncompiled_results\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError analysis for compiled question decomposition:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m present_errors(compiled_results)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'uncompiled_results' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Error analysis for uncompiled question decomposition:\")\n",
    "present_errors(uncompiled_results)\n",
    "\n",
    "print(\"Error analysis for compiled question decomposition:\")\n",
    "present_errors(compiled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
