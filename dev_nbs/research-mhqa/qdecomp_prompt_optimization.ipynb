{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-70b-tgi\",\n",
    "    temperature=0.7,\n",
    "    cache=False,\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "        num_rows: 19938\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable'],\n",
       "        num_rows: 2417\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dsd = load_dataset('bdsaglam/musique', 'answerable')\n",
    "dsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function that samples from the dataset with equal distribution of n_hops\n",
    "def sample_evenly(dataset, n_samples):\n",
    "    n_hops = np.unique(dataset['n_hops'])\n",
    "    samples_per_hop = n_samples // len(n_hops)\n",
    "    for hop in n_hops:\n",
    "        hop_samples = dataset.filter(lambda x: len(x['question_decomposition']) == hop).shuffle().select(range(samples_per_hop))\n",
    "        yield from hop_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = list(sample_evenly(dsd['train'], 100))\n",
    "val_samples = list(sample_evenly(dsd['validation'], 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example(record):\n",
    "    decomposition = '\\n'.join([f\"{i+1}. {item['question']}\" for i, item in enumerate(record[\"question_decomposition\"])])\n",
    "    return dspy.Example(\n",
    "        question=record[\"question\"],\n",
    "        decomposition=decomposition,\n",
    "    ).with_inputs(\"question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example({'question': \"Who does the actor who played Schindler in the movie Schindler's List play in Star Wars 1?\", 'decomposition': \"1. who played schindler in the movie schindler's list\\n2. who does #1 play in star wars 1\"}) (input_keys={'question'})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_example(train_samples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = [make_example(record) for record in train_samples]\n",
    "valset = [make_example(record) for record in val_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Who does the actor who played Schindler in the movie Schindler's List play in Star Wars 1?\",\n",
       " 'decomposition': \"1. who played schindler in the movie schindler's list\\n2. who does #1 play in star wars 1\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_example(example):\n",
    "    print(example.question)\n",
    "    print(example.decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did the Soviet Union seal off the city where the author of Hotel Savoy worked during the Weimar Republic?\n",
      "1. Hotel Savoy >> author\n",
      "2. #1 >> work location\n",
      "3. when the soviet union sealed off the city of #2\n",
      "\n",
      "In 2014, what was the unemployment rate in the country separated by the Rhine from the country where Aschenbrodel's composer was a citizen?\n",
      "1. AschenbrÃ¶del >> composer\n",
      "2. #1 >> country of citizenship\n",
      "3. The Rhine forms the border between #2 and what other country?\n",
      "4. What was the unemployment rate in #3 in 2014?\n",
      "\n",
      "There was a proposal to connect rural Alaska to the continent where Nguyen Van Nghi was born. What year was this project announced?\n",
      "1. Nguyen Van Nghi >> place of birth\n",
      "2. What continent is #1 found on?\n",
      "3. In what year was a project to connect #2 and rural Alaska announced?\n",
      "\n",
      "What government followed the rule of the monarch who re-translated the Reflections into French, in the country that holds the Canton of Coussey?\n",
      "1. Who re-translated the Reflections into French?\n",
      "2. Coussey >> country\n",
      "3. #1 of #2 >> followed by\n",
      "\n",
      "What was named the Trusteeship of the Powerful by the organization in which the US and the country the US beat in the Miracle on Ice became important members?\n",
      "1. who did the us beat in the miracle on ice\n",
      "2. the us and #1 became important members in\n",
      "3. What did The #2 name \"The Trusteeship Of The Powerful\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for example in random.sample(trainset,5):\n",
    "    print_example(example)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "## Exact Match\n",
    "def split_subquestions(decomposition_str):\n",
    "    for line in decomposition_str.split(\"\\n\"):\n",
    "        if line.strip():\n",
    "            parts = line.split(\". \", 1)\n",
    "            if len(parts) == 1:\n",
    "                return parts[0].strip\n",
    "            elif len(parts) == 2:\n",
    "                yield parts[1].strip()\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid decomposition line: {line}\")\n",
    "\n",
    "\n",
    "# Update the evaluation function\n",
    "def evaluate_decomposition_exact_match(example, pred, trace=None):\n",
    "    gold_sub_questions = list(split_subquestions(example.decomposition))\n",
    "    pred_sub_questions = list(split_subquestions(pred.decomposition))\n",
    "\n",
    "    assert len(gold_sub_questions), \"Gold decomposition is empty.\"\n",
    "\n",
    "    exact_matches = len([1 for gold, pred in zip(gold_sub_questions, pred_sub_questions) if gold == pred])\n",
    "    accuracy = exact_matches / len(gold_sub_questions)\n",
    "    return accuracy\n",
    "\n",
    "## LLM as Judge\n",
    "class DecompositionJudge(dspy.Signature):\n",
    "    \"\"\"Judge whether the predicted decomposition matches the ground truth.\n",
    "\n",
    "    Instructions:\n",
    "    - Given a ground-truth decomposition and a predicted decomposition, assess whether they are equivalent in meaning.\n",
    "    - Consider whether the steps correspond logically, even if worded differently.\n",
    "    - Output 'Yes' if they are equivalent, 'No' otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    ground_truth: str = dspy.InputField(desc=\"The ground-truth decomposition\")\n",
    "    prediction: str = dspy.InputField(desc=\"The predicted decomposition\")\n",
    "    equivalent: str = dspy.OutputField(desc=\"Are the decompositions equivalent? [Yes/No]\", prefix=\"Equivalent[Yes/No]:\")\n",
    "\n",
    "qdecomp_judge = dspy.Predict(DecompositionJudge)\n",
    "\n",
    "# Updated evaluation function using the judge\n",
    "def evaluate_decomposition_llm(example, pred, trace=None):\n",
    "    result = qdecomp_judge(\n",
    "        ground_truth=example.decomposition,\n",
    "        prediction=pred.decomposition,\n",
    "    )\n",
    "    is_equivalent = result.equivalent.strip().lower()\n",
    "    return int(is_equivalent == \"yes\")\n",
    "\n",
    "\n",
    "## Combined\n",
    "\n",
    "def evaluate_decomposition(example, pred, trace=None):\n",
    "    accuracy = evaluate_decomposition_exact_match(example, pred, trace)\n",
    "    if accuracy >= 0.8:\n",
    "        return accuracy\n",
    "    return evaluate_decomposition_llm(example, pred, trace)\n",
    "\n",
    "\n",
    "# Set up the evaluation function\n",
    "evaluate_qd = Evaluate(devset=valset, metric=evaluate_decomposition, num_threads=4, display_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecomposeQuestion(dspy.Signature):\n",
    "    \"\"\"Decompose a complex question into simpler sub-questions.\"\"\"\n",
    "\n",
    "    question: str = dspy.InputField()\n",
    "    decomposition: str = dspy.OutputField(\n",
    "        desc=\"Enumerated list of sub-questions, using '#n >>' notation for dependent questions\"\n",
    "    )\n",
    "\n",
    "class QuestionDecompositionModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.decompose = dspy.Predict(DecomposeQuestion)\n",
    "\n",
    "    def forward(self, question):\n",
    "        pred = self.decompose(question=question)\n",
    "        return dspy.Prediction(decomposition=pred.decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the uncompiled question decomposition module\n",
    "uncompiled_qd = QuestionDecompositionModule()\n",
    "\n",
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score = evaluate_qd(uncompiled_qd)\n",
    "print(f\"Uncompiled Question Decomposition Score: {uncompiled_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 16 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/99 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 86 / 99  (86.9): 100%|ââââââââââ| 99/99 [12:36<00:00,  7.64s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 86.87 for seed -3\n",
      "Scores so far: [86.87]\n",
      "Best score so far: 86.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 80.0 / 99  (80.8): 100%|ââââââââââ| 99/99 [14:28<00:00,  8.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81]\n",
      "Best score so far: 86.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â         | 10/99 [07:16<1:04:44, 43.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 11 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 87.0 / 99  (87.9): 100%|ââââââââââ| 99/99 [14:57<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 87.88 for seed -1\n",
      "Scores so far: [86.87, 80.81, 87.88]\n",
      "Best score so far: 87.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â         | 7/99 [04:48<1:03:05, 41.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 84 / 99  (84.8): 100%|ââââââââââ| 99/99 [14:03<00:00,  8.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85]\n",
      "Best score so far: 87.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â         | 3/99 [02:10<1:09:50, 43.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 84 / 99  (84.8): 100%|ââââââââââ| 99/99 [13:54<00:00,  8.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85]\n",
      "Best score so far: 87.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/99 [00:48<1:18:43, 48.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 88 / 99  (88.9): 100%|ââââââââââ| 99/99 [14:03<00:00,  8.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 88.89 for seed 2\n",
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89]\n",
      "Best score so far: 88.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â         | 4/99 [02:55<1:09:29, 43.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 81 / 99  (81.8): 100%|ââââââââââ| 99/99 [11:07<00:00,  6.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82]\n",
      "Best score so far: 88.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â         | 5/99 [01:25<26:42, 17.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 84.0 / 99  (84.8): 100%|ââââââââââ| 99/99 [08:14<00:00,  5.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85]\n",
      "Best score so far: 88.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â         | 6/99 [01:40<26:00, 16.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 89 / 99  (89.9): 100%|ââââââââââ| 99/99 [08:32<00:00,  5.17s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 89.9 for seed 5\n",
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9]\n",
      "Best score so far: 89.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â         | 2/99 [00:41<33:56, 20.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 87 / 99  (87.9): 100%|ââââââââââ| 99/99 [08:49<00:00,  5.35s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88]\n",
      "Best score so far: 89.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â         | 8/99 [03:36<41:05, 27.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 81 / 99  (81.8): 100%|ââââââââââ| 99/99 [08:30<00:00,  5.16s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88, 81.82]\n",
      "Best score so far: 89.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â         | 5/99 [01:40<31:34, 20.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 86 / 99  (86.9): 100%|ââââââââââ| 99/99 [08:20<00:00,  5.05s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88, 81.82, 86.87]\n",
      "Best score so far: 89.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â         | 9/99 [02:49<28:16, 18.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 88 / 99  (88.9): 100%|ââââââââââ| 99/99 [08:11<00:00,  4.96s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88, 81.82, 86.87, 88.89]\n",
      "Best score so far: 89.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/99 [00:15<24:46, 15.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 80.0 / 99  (80.8): 100%|ââââââââââ| 99/99 [06:20<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88, 81.82, 86.87, 88.89, 80.81]\n",
      "Best score so far: 89.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â         | 8/99 [01:35<18:08, 11.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 88.0 / 99  (88.9): 100%|ââââââââââ| 99/99 [04:43<00:00,  2.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88, 81.82, 86.87, 88.89, 80.81, 88.89]\n",
      "Best score so far: 89.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â         | 9/99 [01:40<16:45, 11.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 92.0 / 99  (92.9): 100%|ââââââââââ| 99/99 [02:10<00:00,  1.32s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 92.93 for seed 12\n",
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88, 81.82, 86.87, 88.89, 80.81, 88.89, 92.93]\n",
      "Best score so far: 92.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â         | 5/99 [00:17<05:26,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 86 / 99  (86.9): 100%|ââââââââââ| 99/99 [02:10<00:00,  1.32s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88, 81.82, 86.87, 88.89, 80.81, 88.89, 92.93, 86.87]\n",
      "Best score so far: 92.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â         | 2/99 [00:07<06:01,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 84 / 99  (84.8): 100%|ââââââââââ| 99/99 [02:10<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88, 81.82, 86.87, 88.89, 80.81, 88.89, 92.93, 86.87, 84.85]\n",
      "Best score so far: 92.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â         | 4/99 [00:15<06:01,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 89 / 99  (89.9): 100%|ââââââââââ| 99/99 [02:09<00:00,  1.31s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [86.87, 80.81, 87.88, 84.85, 84.85, 88.89, 81.82, 84.85, 89.9, 87.88, 81.82, 86.87, 88.89, 80.81, 88.89, 92.93, 86.87, 84.85, 89.9]\n",
      "Best score so far: 92.93\n",
      "19 candidate programs found.\n",
      "Question Decomposition module compiled and optimized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShot, LabeledFewShot, BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Set up the teleprompter\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(\n",
    "    metric=evaluate_decomposition, \n",
    "    max_bootstrapped_demos=8, \n",
    "    max_labeled_demos=8,\n",
    ")\n",
    "\n",
    "# Compile and optimize the question decomposition module\n",
    "compiled_qd = teleprompter.compile(uncompiled_qd, trainset=trainset, valset=valset)\n",
    "compiled_qd.save('compiled-qd.json')\n",
    "print(\"Question Decomposition module compiled and optimized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 82 / 99  (82.8): 100%|ââââââââââ| 99/99 [16:02<00:00,  9.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Question Decomposition Score: 82.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score = evaluate_qd(uncompiled_qd)\n",
    "print(f\"Uncompiled Question Decomposition Score: {uncompiled_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 87 / 99  (87.9): 100%|ââââââââââ| 99/99 [02:35<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiled Question Decomposition Module Score: 87.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the compiled question decomposition module\n",
    "compiled_score = evaluate_qd(compiled_qd)\n",
    "print(f\"Compiled Question Decomposition Score: {compiled_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('decompose', Predict(DecomposeQuestion(question -> decomposition\n",
      "    instructions='Decompose a complex question into simpler sub-questions.'\n",
      "    question = Field(annotation=str required=True json_schema_extra={'__dspy_field_type': 'input', 'prefix': 'Question:', 'desc': '${question}'})\n",
      "    decomposition = Field(annotation=str required=True json_schema_extra={'desc': \"Enumerated list of sub-questions, using '#n >>' notation for dependent questions\", '__dspy_field_type': 'output', 'prefix': 'Decomposition:'})\n",
      ")))]\n"
     ]
    }
   ],
   "source": [
    "compiled_qd.save('compiled-qd.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error analysis for compiled Question Decomposition module:\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis\n",
    "def perform_error_analysis(qd_module, dataset, num_samples=5):\n",
    "    errors = []\n",
    "    for example in dataset:\n",
    "        pred = qd_module(question=example.question)\n",
    "        if not evaluate_decomposition(example, pred):\n",
    "            errors.append((example, pred))\n",
    "    \n",
    "    print(f\"Total errors: {len(errors)}\")\n",
    "    print(\"\\nSample of errors:\")\n",
    "    for example, pred in errors[:num_samples]:\n",
    "        print(f\"Original Question: {example.question}\")\n",
    "        print(f\"# True Decomposition\\n{example.decomposition}\")\n",
    "        print(f\"# Predicted Decomposition\\n{pred.decomposition}\")\n",
    "        print()\n",
    "\n",
    "print(\"\\nError analysis for compiled Question Decomposition module:\")\n",
    "perform_error_analysis(compiled_qd, valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_qd.predictors()[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
