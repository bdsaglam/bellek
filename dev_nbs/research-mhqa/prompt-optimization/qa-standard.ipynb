{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Question Answering Pipeline\n",
    "\n",
    "This notebook implements a DSPy pipeline for optimizing question answering prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bellek.utils import set_seed\n",
    "set_seed(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-70b-tgi\",\n",
    "    temperature=0.1,\n",
    "    cache=False,\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable', 'n_hops'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable', 'n_hops'],\n",
       "     num_rows: 300\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_ds = load_dataset('bdsaglam/musique-mini', 'answerable', split='train')\n",
    "val_ds = load_dataset('bdsaglam/musique-mini', 'answerable', split='validation')\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_paragraph(paragraph):\n",
    "    text = paragraph['paragraph_text']\n",
    "    title = paragraph['title']\n",
    "    return f\"# {title}\\n{text}\"\n",
    "\n",
    "def make_example(record):\n",
    "    supporting_paragraphs = [p for p in record['paragraphs'] if p['is_supporting']]\n",
    "    context = \"\\n\\n\".join([format_paragraph(p) for p in supporting_paragraphs])\n",
    "    return dspy.Example(\n",
    "        question=record['question'],\n",
    "        context=context,\n",
    "        answer=record['answer'],\n",
    "        answers=[record['answer'], *record['answer_aliases']],\n",
    "    ).with_inputs('question', 'context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = [make_example(record) for record in train_ds]\n",
    "valset = [make_example(record) for record in val_ds]\n",
    "len(trainset), len(valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Signatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer the question based on the given context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = dspy.Predict(GenerateAnswer)\n",
    "\n",
    "    def forward(self, context, question):\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the uncompiled QA module\n",
    "uncompiled_program = QAModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the optimization metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/pc/.cache/huggingface/modules/evaluate_modules/metrics/bdsaglam--musique/9f409241d4cc6ea7853124e79cf44954a75900a0a2c0b9d20b909c2396f6b071 (last modified on Tue Jul 23 21:54:03 2024) since it couldn't be found locally at bdsaglam--musique, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from bellek.musique.eval import calculate_metrics\n",
    "\n",
    "def compute_scores(results):\n",
    "    df = pd.DataFrame([{**dict(example), \"predicted_answer\": pred.answer} for example, pred, score in results])\n",
    "    return calculate_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import answer_exact_match_str\n",
    "\n",
    "def evaluate_answer(example, pred, trace=None):\n",
    "    return answer_exact_match_str(pred.answer, example.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_program = Evaluate(\n",
    "    metric=evaluate_answer,\n",
    "    devset=valset,\n",
    "    num_threads=32,\n",
    "    display_progress=True,\n",
    "    return_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 164 / 300  (54.7): 100%|██████████| 300/300 [02:34<00:00,  1.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Question Answering Scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.5466666666666666,\n",
       " 'f1': 0.6624913974913975,\n",
       " 'fuzzy_match': 0.6033333333333334}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score, uncompiled_results = evaluate_program(uncompiled_program)\n",
    "print(\"Uncompiled Question Answering Scores\")\n",
    "compute_scores(uncompiled_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import LabeledFewShot, BootstrapFewShot, BootstrapFewShotWithRandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to bootstrap 10 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 176 / 300  (58.7): 100%|██████████| 300/300 [09:50<00:00,  1.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 58.67 for seed -3\n",
      "Scores so far: [58.67]\n",
      "Best score so far: 58.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 188 / 300  (62.7): 100%|██████████| 300/300 [19:59<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 62.67 for seed -2\n",
      "Scores so far: [58.67, 62.67]\n",
      "Best score so far: 62.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 11/300 [00:56<24:55,  5.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 184 / 300  (61.3): 100%|██████████| 300/300 [12:57<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [58.67, 62.67, 61.33]\n",
      "Best score so far: 62.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 12/300 [01:04<25:58,  5.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 13 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 185 / 300  (61.7): 100%|██████████| 300/300 [15:09<00:00,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [58.67, 62.67, 61.33, 61.67]\n",
      "Best score so far: 62.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/300 [00:22<27:39,  5.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 187 / 300  (62.3): 100%|██████████| 300/300 [18:37<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [58.67, 62.67, 61.33, 61.67, 62.33]\n",
      "Best score so far: 62.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:10<25:54,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 108 / 163  (66.3):  54%|█████▍    | 163/300 [13:27<23:24, 10.25s/it]"
     ]
    }
   ],
   "source": [
    "teleprompter_cls = BootstrapFewShotWithRandomSearch\n",
    "\n",
    "teleprompter = teleprompter_cls(\n",
    "    metric=evaluate_answer,\n",
    "    max_bootstrapped_demos=8, \n",
    "    max_labeled_demos=8,\n",
    "    num_threads=4,\n",
    "    num_candidate_programs=10\n",
    ")\n",
    "\n",
    "compiled_program = teleprompter.compile(uncompiled_program, trainset=trainset)\n",
    "compiled_program_filename = f\"compiled-qa-standard-{teleprompter_cls.__name__.lower()}.json\"\n",
    "compiled_program.save(compiled_program_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 187 / 300  (62.3): 100%|██████████| 300/300 [10:45<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BootstrapFewShotWithRandomSearch Compiled Question Answering Scores\n",
      "{'exact_match': 0.6233333333333333, 'f1': 0.7193650645650645, 'fuzzy_match': 0.6766666666666666}\n"
     ]
    }
   ],
   "source": [
    "_, compiled_results = evaluate_program(compiled_program)\n",
    "print(f\"{teleprompter_cls.__name__} Compiled Question Answering Scores\")\n",
    "print(compute_scores(compiled_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_errors(results):\n",
    "    errors = [(example, pred) for example, pred, score in results if float(score) < 0.9] \n",
    "    for example, pred in errors:\n",
    "        print(f\"Question: {example.question}\")\n",
    "        print(f\"Context: {example.context}\")\n",
    "        print(f\"Groundtruth Answers: {example.answers}\")\n",
    "        print(f\"Predicted Answer: {pred.answer}\")\n",
    "        print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error analysis for uncompiled program\\n\\n\")\n",
    "present_errors(uncompiled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error analysis for compiled program:\")\n",
    "present_errors(compiled_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "example = trainset[i]\n",
    "pred = compiled_program(context=example.context, question=example.question)\n",
    "example.answers, pred.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
