{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSPy Question Answering Pipeline\n",
    "\n",
    "This notebook implements a DSPy pipeline for optimizing question answering prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import dspy\n",
    "from dspy.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bellek.utils import set_seed\n",
    "set_seed(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dspy.LM(\n",
    "    \"openai/llama-3-70b-tgi\",\n",
    "    temperature=0.1,\n",
    "    cache=False,\n",
    "    api_base=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "dspy.configure(lm=lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable', 'n_hops'],\n",
       "     num_rows: 300\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['id', 'paragraphs', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'answerable', 'n_hops'],\n",
       "     num_rows: 300\n",
       " }))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_ds = load_dataset('bdsaglam/musique-mini', 'answerable', split='train')\n",
    "val_ds = load_dataset('bdsaglam/musique-mini', 'answerable', split='validation')\n",
    "train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_paragraph(paragraph):\n",
    "    text = paragraph['paragraph_text']\n",
    "    title = paragraph['title']\n",
    "    return f\"# {title}\\n{text}\"\n",
    "\n",
    "def make_example(record):\n",
    "    supporting_paragraphs = [p for p in record['paragraphs'] if p['is_supporting']]\n",
    "    context = \"\\n\\n\".join([format_paragraph(p) for p in supporting_paragraphs])\n",
    "    return dspy.Example(\n",
    "        question=record['question'],\n",
    "        context=context,\n",
    "        answer=record['answer'],\n",
    "        answers=[record['answer'], *record['answer_aliases']],\n",
    "    ).with_inputs('question', 'context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = [make_example(record) for record in train_ds]\n",
    "valset = [make_example(record) for record in val_ds]\n",
    "len(trainset), len(valset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateAnswer(dspy.Signature):\n",
    "    \"\"\"Answer the question based on the given context.\"\"\"\n",
    "    context = dspy.InputField(desc=\"may contain relevant facts\")\n",
    "    question = dspy.InputField()\n",
    "    answer = dspy.OutputField(desc=\"often between 1 and 5 words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "from dspy.primitives.program import Module\n",
    "from dspy.signatures.signature import ensure_signature\n",
    "\n",
    "\n",
    "class ConnectTheEntities(Module):\n",
    "    def __init__(self, signature, rationale_type=None, activated=True, **config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activated = activated\n",
    "\n",
    "        self.signature = signature = ensure_signature(signature)\n",
    "\n",
    "        prefix = \"Let's identify the relevant entity-relation-entity triples in the format of 'subj;relation;obj'\\n\"\n",
    "        desc = \"${triples}\"\n",
    "        rationale_type = rationale_type or dspy.OutputField(prefix=prefix, desc=desc)\n",
    "\n",
    "        # Add \"rationale\" field to the output signature.\n",
    "        extended_signature = signature.prepend(\"triples\", rationale_type, type_=str)\n",
    "        \n",
    "        self._predict = dspy.Predict(extended_signature, **config)\n",
    "        self._predict.extended_signature = extended_signature\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        assert self.activated in [True, False]\n",
    "\n",
    "        signature = kwargs.pop(\"new_signature\", self._predict.extended_signature if self.activated else self.signature)\n",
    "        return self._predict(signature=signature, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def demos(self):\n",
    "        return self._predict.demos\n",
    "\n",
    "    @property\n",
    "    def extended_signature(self):\n",
    "        return self._predict.extended_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAModule(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.generate_answer = ConnectTheEntities(GenerateAnswer)\n",
    "\n",
    "    def forward(self, context, question):\n",
    "        prediction = self.generate_answer(context=context, question=question)\n",
    "        return dspy.Prediction(context=context, answer=prediction.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the uncompiled QA module\n",
    "uncompiled_program = QAModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the optimization metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/pc/.cache/huggingface/modules/evaluate_modules/metrics/bdsaglam--musique/9f409241d4cc6ea7853124e79cf44954a75900a0a2c0b9d20b909c2396f6b071 (last modified on Tue Jul 23 21:54:03 2024) since it couldn't be found locally at bdsaglam--musique, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from bellek.musique.eval import calculate_metrics\n",
    "\n",
    "def compute_scores(results):\n",
    "    df = pd.DataFrame([{**dict(example), \"predicted_answer\": pred.answer} for example, pred, score in results])\n",
    "    return calculate_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.evaluate import answer_exact_match_str\n",
    "\n",
    "def evaluate_answer(example, pred, trace=None):\n",
    "    return answer_exact_match_str(pred.answer, example.answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_program = Evaluate(\n",
    "    metric=evaluate_answer,\n",
    "    devset=valset,\n",
    "    num_threads=32,\n",
    "    display_progress=True,\n",
    "    return_outputs=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 179 / 300  (59.7): 100%|██████████| 300/300 [02:33<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uncompiled Question Answering Scores\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.5966666666666667,\n",
       " 'f1': 0.6866563414470185,\n",
       " 'fuzzy_match': 0.6433333333333333}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the uncompiled question decomposition module\n",
    "uncompiled_score, uncompiled_results = evaluate_program(uncompiled_program)\n",
    "print(\"Uncompiled Question Answering Scores\")\n",
    "compute_scores(uncompiled_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 4 traces per predictor.\n",
      "Will attempt to bootstrap 10 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 193 / 300  (64.3): 100%|██████████| 300/300 [04:59<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 64.33 for seed -3\n",
      "Scores so far: [64.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 193 / 300  (64.3): 100%|██████████| 300/300 [05:02<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [64.33, 64.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/300 [00:13<13:36,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 193 / 300  (64.3): 100%|██████████| 300/300 [08:30<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [64.33, 64.33, 64.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 6/300 [00:14<11:32,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 7 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 187 / 300  (62.3): 100%|██████████| 300/300 [08:32<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [64.33, 64.33, 64.33, 62.33]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/300 [00:07<12:46,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 183 / 300  (61.0): 100%|██████████| 300/300 [07:46<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [64.33, 64.33, 64.33, 62.33, 61.0]\n",
      "Best score so far: 64.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:05<13:22,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 195 / 300  (65.0): 100%|██████████| 300/300 [07:44<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best score: 65.0 for seed 2\n",
      "Scores so far: [64.33, 64.33, 64.33, 62.33, 61.0, 65.0]\n",
      "Best score so far: 65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 5/300 [00:13<13:42,  2.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 186 / 300  (62.0): 100%|██████████| 300/300 [07:04<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [64.33, 64.33, 64.33, 62.33, 61.0, 65.0, 62.0]\n",
      "Best score so far: 65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/300 [00:06<16:17,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 3 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 187 / 300  (62.3): 100%|██████████| 300/300 [07:49<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [64.33, 64.33, 64.33, 62.33, 61.0, 65.0, 62.0, 62.33]\n",
      "Best score so far: 65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/300 [00:07<12:53,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 4 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 193 / 300  (64.3): 100%|██████████| 300/300 [27:10<00:00,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [64.33, 64.33, 64.33, 62.33, 61.0, 65.0, 62.0, 62.33, 64.33]\n",
      "Best score so far: 65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/300 [00:23<1:56:43, 23.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 182 / 300  (60.7): 100%|██████████| 300/300 [29:31<00:00,  5.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores so far: [64.33, 64.33, 64.33, 62.33, 61.0, 65.0, 62.0, 62.33, 64.33, 60.67]\n",
      "Best score so far: 65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 4/300 [00:56<1:09:34, 14.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 187 / 297  (63.0):  99%|█████████▉| 297/300 [39:22<00:36, 12.33s/it]"
     ]
    }
   ],
   "source": [
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "teleprompter_cls = BootstrapFewShotWithRandomSearch\n",
    "teleprompter = teleprompter_cls(\n",
    "    metric=evaluate_answer,\n",
    "    max_labeled_demos=0,\n",
    "    max_bootstrapped_demos=4,\n",
    "    num_threads=4,\n",
    "    num_candidate_programs=10\n",
    ")\n",
    "\n",
    "compiled_program = teleprompter.compile(uncompiled_program, trainset=trainset)\n",
    "compiled_program_filename = f\"compiled-qa-cte-{teleprompter_cls.__name__.lower()}.json\"\n",
    "compiled_program.save(compiled_program_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 182 / 300  (60.7): 100%|██████████| 300/300 [06:12<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BootstrapFewShotWithRandomSearch Compiled Question Answering Scores\n",
      "{'exact_match': 0.6066666666666667, 'f1': 0.7130639399586769, 'fuzzy_match': 0.6766666666666666}\n"
     ]
    }
   ],
   "source": [
    "_, compiled_results = evaluate_program(compiled_program)\n",
    "print(f\"{teleprompter_cls.__name__} Compiled Question Answering Scores\")\n",
    "print(compute_scores(compiled_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_errors(results):\n",
    "    errors = [(example, pred) for example, pred, score in results if float(score) < 0.9] \n",
    "    for example, pred in errors:\n",
    "        print(f\"Question: {example.question}\")\n",
    "        print(f\"Context: {example.context}\")\n",
    "        print(f\"Groundtruth Answers: {example.answers}\")\n",
    "        print(f\"Predicted Answer: {pred.answer}\")\n",
    "        print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error analysis for uncompiled program\\n\\n\")\n",
    "present_errors(uncompiled_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Error analysis for compiled program:\")\n",
    "present_errors(compiled_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "example = trainset[i]\n",
    "pred = compiled_program(context=example.context, question=example.question)\n",
    "example.answers, pred.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.history[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
