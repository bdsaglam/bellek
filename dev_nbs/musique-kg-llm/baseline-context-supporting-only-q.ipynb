{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bellek.text.utils import fuzzy_match\n",
    "from bellek.utils import set_seed, jprint\n",
    "\n",
    "set_seed(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbdsaglam\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/bdsaglam/dev/repos/bellek/dev_nbs/musique-kg-llm/wandb/run-20240625_120931-qiwej0vn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context/runs/qiwej0vn' target=\"_blank\">smart-deluge-2</a></strong> to <a href='https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context' target=\"_blank\">https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context/runs/qiwej0vn' target=\"_blank\">https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context/runs/qiwej0vn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "from bellek.langchain.obs import patch_wandb_tracer_serialize_io\n",
    "from bellek.wandb import generate_run_id\n",
    "\n",
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"thesis-mhqa-baseline-context\"\n",
    "os.environ[\"WANDB_RUN_ID\"] = generate_run_id(8)\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = os.path.basename(globals()['__vsc_ipynb_file__'])\n",
    "os.environ[\"WANDB_NOTES\"] = \"Model sees only question\"\n",
    "\n",
    "patch_wandb_tracer_serialize_io()\n",
    "\n",
    "wandb_run = wandb.init(project=os.environ[\"WANDB_PROJECT\"], resume=os.environ[\"WANDB_RUN_ID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_aliases</th>\n",
       "      <th>answerable</th>\n",
       "      <th>question</th>\n",
       "      <th>question_decomposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2hop__131818_161450</td>\n",
       "      <td>[{'idx': 0, 'title': 'Maria Carrillo High Scho...</td>\n",
       "      <td>in the north-east of the country south of the ...</td>\n",
       "      <td>[Caspian Sea]</td>\n",
       "      <td>True</td>\n",
       "      <td>Where is the Voshmgir District located?</td>\n",
       "      <td>[{'question': 'Which country is the Voshmgir D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2hop__444265_82341</td>\n",
       "      <td>[{'idx': 0, 'title': 'Ocala, Florida', 'paragr...</td>\n",
       "      <td>in Northern Florida</td>\n",
       "      <td>[Northern Florida]</td>\n",
       "      <td>True</td>\n",
       "      <td>In what part of Florida is Tom Denney's birthp...</td>\n",
       "      <td>[{'question': 'Where is Tom Denney's birthplac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2hop__711946_269414</td>\n",
       "      <td>[{'idx': 0, 'title': 'Wild Thing (Tone Lōc son...</td>\n",
       "      <td>Kill Rock Stars</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>What record label is the performer who release...</td>\n",
       "      <td>[{'question': 'Who is the performer that relea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2hop__311931_417706</td>\n",
       "      <td>[{'idx': 0, 'title': 'The Main Attraction (alb...</td>\n",
       "      <td>Attic Records</td>\n",
       "      <td>[Attic]</td>\n",
       "      <td>True</td>\n",
       "      <td>What record label does the performer of Emotio...</td>\n",
       "      <td>[{'question': 'Who is the performer of Emotion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2hop__809785_606637</td>\n",
       "      <td>[{'idx': 0, 'title': 'The Main Attraction (alb...</td>\n",
       "      <td>Secret City Records</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>What record label does the performer of Advent...</td>\n",
       "      <td>[{'question': 'Who is the performer of Adventu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                         paragraphs  \\\n",
       "0  2hop__131818_161450  [{'idx': 0, 'title': 'Maria Carrillo High Scho...   \n",
       "1   2hop__444265_82341  [{'idx': 0, 'title': 'Ocala, Florida', 'paragr...   \n",
       "2  2hop__711946_269414  [{'idx': 0, 'title': 'Wild Thing (Tone Lōc son...   \n",
       "3  2hop__311931_417706  [{'idx': 0, 'title': 'The Main Attraction (alb...   \n",
       "4  2hop__809785_606637  [{'idx': 0, 'title': 'The Main Attraction (alb...   \n",
       "\n",
       "                                              answer      answer_aliases  \\\n",
       "0  in the north-east of the country south of the ...       [Caspian Sea]   \n",
       "1                                in Northern Florida  [Northern Florida]   \n",
       "2                                    Kill Rock Stars                  []   \n",
       "3                                      Attic Records             [Attic]   \n",
       "4                                Secret City Records                  []   \n",
       "\n",
       "   answerable                                           question  \\\n",
       "0        True            Where is the Voshmgir District located?   \n",
       "1        True  In what part of Florida is Tom Denney's birthp...   \n",
       "2        True  What record label is the performer who release...   \n",
       "3        True  What record label does the performer of Emotio...   \n",
       "4        True  What record label does the performer of Advent...   \n",
       "\n",
       "                              question_decomposition  \n",
       "0  [{'question': 'Which country is the Voshmgir D...  \n",
       "1  [{'question': 'Where is Tom Denney's birthplac...  \n",
       "2  [{'question': 'Who is the performer that relea...  \n",
       "3  [{'question': 'Who is the performer of Emotion...  \n",
       "4  [{'question': 'Who is the performer of Adventu...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df = pd.read_json('../../data/generated/musique-evaluation/dataset.jsonl', orient='records', lines=True)\n",
    "qd_df = pd.read_json('../../data/generated/musique-evaluation/question-decomposition.jsonl', orient='records', lines=True)\n",
    "df = pd.merge(ds_df.drop(columns=['question', 'question_decomposition']), qd_df, on='id', suffixes=('', ''))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_docs(example, only_supporting=False):\n",
    "    ps = example[\"paragraphs\"]\n",
    "    for p in ps:\n",
    "        if only_supporting and not p[\"is_supporting\"]:\n",
    "            continue\n",
    "        idx = p[\"idx\"]\n",
    "        title = p[\"title\"]\n",
    "        body = p[\"paragraph_text\"]\n",
    "        is_supporting = p[\"is_supporting\"]\n",
    "        text = f\"# {title}\\n{body}\"\n",
    "        yield dict(\n",
    "            text=text,\n",
    "            metadata={\"parent_id\": example[\"id\"], \"idx\": idx, \"is_supporting\": is_supporting},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_example(example, predicted_answer):\n",
    "    text = \"\\n\\n\".join([p[\"paragraph_text\"] for p in example['paragraphs']])\n",
    "    print(\"=\"*80)\n",
    "    print(\"Question:\", example[\"question\"])\n",
    "    print(\"Reference Answer:\", example['answer'])\n",
    "    print(\"Predicted Answer:\", predicted_answer)\n",
    "    print(\"-\"*80)\n",
    "    print(\"Paragraphs\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bdsaglam/dev/repos/bellek/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "/Users/bdsaglam/dev/repos/bellek/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: LangChain has introduced a method called `with_structured_output` that is available on ChatModels capable of tool calling. You can read more about the method here: https://python.langchain.com/docs/modules/model_io/chat/structured_output/ Please follow our extraction use case documentation for more guidelines on how to do information extraction with LLMs. https://python.langchain.com/docs/use_cases/extraction/. If you notice other issues, please provide feedback here: https://github.com/langchain-ai/langchain/discussions/18154\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are an expert Q&A system that is trusted around the world. You are given a question that requires multi-hop reasoning. Always answer the query using the provided context information, and not prior knowledge.\n",
    "Some rules to follow:\n",
    "1. Never directly reference the given context in your answer.\n",
    "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
    "3. Your answer must be 2-4 words long.\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the question.\n",
    "{query_str}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT), \n",
    "    (\"user\", USER_PROMPT),\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "class Output(BaseModel):\n",
    "    \"\"\"Output containing the answers for questions.\"\"\"\n",
    "    answer: str\n",
    "    reasoning: str = Field(description=\"Multi-hop reasoning for the answer.\")\n",
    "\n",
    "chain = create_structured_output_runnable(Output, llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_question(example):\n",
    "    return example['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(example):\n",
    "    documents = list(make_docs(example, only_supporting=False))\n",
    "    context = \"\\n\\n\".join([doc[\"text\"] for doc in documents])\n",
    "    output = chain.invoke({\"context_str\": context, \"query_str\": format_question(example)}).dict()\n",
    "    example['predicted_answer'] = output.get(\"answer\")\n",
    "    example['raw_llm_output'] = output\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_answer(example):\n",
    "    try:\n",
    "        return answer(example)\n",
    "    except Exception as exc:\n",
    "        id = example['id']\n",
    "        print(f\"Failed to answer the question {id}\\n{exc}\")\n",
    "        example['predicted_answer'] = None\n",
    "        example['raw_llm_output'] = None\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.globals import set_debug\n",
    "# set_debug(True)\n",
    "\n",
    "# i = 0\n",
    "# example = df.iloc[i].to_dict()\n",
    "# example_ = answer(example)\n",
    "# print(\"Question:\", example['question'])\n",
    "# print(\"Reference answer:\", example['answer'])\n",
    "# print(\"Predicted answer:\", example_['predicted_answer'])\n",
    "\n",
    "# print(\"-\"*20)\n",
    "# jprint(example_['raw_llm_output'])\n",
    "\n",
    "# set_debug(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(safe_answer, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_musique_scores(dataf: pd.DataFrame) -> dict:\n",
    "    metric = evaluate.load(\"bdsaglam/musique\")\n",
    "    predictions = dataf[\"predicted_answer\"].tolist()\n",
    "    references = dataf.apply(lambda row: [row[\"answer\"], *row[\"answer_aliases\"]], axis=1).tolist()\n",
    "    scores = metric.compute(predictions=predictions, references=references)\n",
    "    return scores\n",
    "\n",
    "def _fuzzy_match(example):\n",
    "    pred, ref = example['predicted_answer'], example['answer']\n",
    "    return pred is not None and ((pred in ref) or (ref in pred) or fuzzy_match(pred, ref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fuzzy_match': 0.395, 'exact_match': 0.345, 'f1': 0.4478809523809524}\n"
     ]
    }
   ],
   "source": [
    "df[\"fuzzy_match\"] = df.apply(_fuzzy_match, axis=1)\n",
    "\n",
    "# log scores\n",
    "scores = {\n",
    "    \"fuzzy_match\": df[\"fuzzy_match\"].mean(),\n",
    "}\n",
    "scores.update(calculate_musique_scores(df))\n",
    "print(scores)\n",
    "wandb_run.log(scores)\n",
    "\n",
    "# log evaluation results\n",
    "cols_to_keep = ['id', 'question', 'question_decomposition', 'answer', 'answer_aliases', 'predicted_answer', 'fuzzy_match', 'raw_llm_output']\n",
    "eval_table = wandb.Table(dataframe=df[cols_to_keep])\n",
    "wandb_run.log({\n",
    "    \"evaluation-table\": eval_table,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b19cf2114214f0b885d71fc8ed53921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.256 MB of 0.256 MB uploaded (0.020 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 6.5%             "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>exact_match</td><td>▁</td></tr><tr><td>f1</td><td>▁</td></tr><tr><td>fuzzy_match</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>exact_match</td><td>0.345</td></tr><tr><td>f1</td><td>0.44788</td></tr><tr><td>fuzzy_match</td><td>0.395</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">smart-deluge-2</strong> at: <a href='https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context/runs/qiwej0vn' target=\"_blank\">https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context/runs/qiwej0vn</a><br/> View project at: <a href='https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context' target=\"_blank\">https://wandb.ai/bdsaglam/thesis-mhqa-baseline-context</a><br/>Synced 6 W&B file(s), 1 media file(s), 8 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240625_120931-qiwej0vn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# finish run\n",
    "wandb_run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
