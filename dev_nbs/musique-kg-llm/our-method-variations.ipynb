{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "from functools import partial\n",
    "\n",
    "from bellek.qa.ablation import answer_question_standard, answer_question_cte\n",
    "from bellek.utils import set_seed, jprint\n",
    "from bellek.musique.singlehop import benchmark as benchmark_single\n",
    "from bellek.musique.multihop import benchmark as benchmark_multi\n",
    "\n",
    "set_seed(89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 2\n",
    "SAMPLE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bellek.musique.constants import ABLATION_RECORD_IDS\n",
    "\n",
    "df = pd.read_json('../../data/generated/musique-evaluation/dataset.jsonl', orient='records', lines=True)\n",
    "df.set_index('id', inplace=True, drop=False)\n",
    "df = df.loc[ABLATION_RECORD_IDS].copy().reset_index(drop=True)\n",
    "df = df.head(SAMPLE_SIZE)\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qd_df = pd.read_json('../../data/generated/musique-evaluation/question-decomposition.jsonl', orient='records', lines=True)\n",
    "df = pd.merge(df.drop(columns=['question', 'question_decomposition']), qd_df, on='id', suffixes=('', ''))\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jerx_file = Path(\"../../data/raw/musique-evaluation/jerx-inferences/llama3-base.jsonl\")\n",
    "jerx_df = pd.read_json(jerx_file, lines=True)\n",
    "jerx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jerx_mapping = {(row['id'], row['paragraph_idx']): row['generation'] for _, row in jerx_df.iterrows()}\n",
    "\n",
    "def extract_triplets(example: dict):\n",
    "    example[\"triplets_str\"] = [jerx_mapping[(example['id'], p['idx'])].strip() for p in example['paragraphs']]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(extract_triplets, axis=1)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"bm25s\").setLevel(logging.ERROR)\n",
    "\n",
    "def bm25_retrieval(docs: list[dict], query: str, top_k: int = 5):\n",
    "    top_k = min(top_k, len(docs))\n",
    "    retriever = bm25s.BM25(corpus=docs)\n",
    "    tokenized_corpus = bm25s.tokenize([doc['text'] for doc in docs], show_progress=False)\n",
    "    retriever.index(tokenized_corpus, show_progress=False)\n",
    "    results, _ = retriever.retrieve(bm25s.tokenize(query), k=top_k, show_progress=False)\n",
    "    return results[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def semantic_retrieval(docs: list[dict], query: str, top_k: int = 5):\n",
    "    embeddings = model.encode([doc['text'] for doc in docs])\n",
    "    query_vectors = model.encode([query])\n",
    "    similarities = model.similarity(embeddings, query_vectors)\n",
    "    sorted_indices = similarities.argsort(dim=0, descending=True)\n",
    "    return [docs[i] for i in sorted_indices[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_retrieval_func = lambda docs,query: docs\n",
    "perfect_retrieval_func = lambda docs,query: [doc for doc in docs if doc['is_supporting']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(1, N_RUNS + 1):\n",
    "    for qdecomp, benchmark in [(False, benchmark_single), (True, benchmark_multi)]:\n",
    "        for qa_technique, qa_func in [('standard', answer_question_standard), ('cte', answer_question_cte)]:\n",
    "            for top_k in [3, 5, 10]:\n",
    "                for retriever_name, retriever in [\n",
    "                        ('bm25', partial(bm25_retrieval, top_k=top_k)), \n",
    "                        ('semantic', partial(semantic_retrieval, top_k=top_k)), \n",
    "                    ]:\n",
    "                    _, scores = benchmark(df, qa_func, retriever, ignore_errors=True)\n",
    "                    results.append({**scores, \"retrieval\": retriever_name, \"top_k\": top_k, \"context\": \"paragraphs\", \"qa\": qa_technique, \"qdecomp\": qdecomp, \"run\": run})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraphs + Triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_paragraphs(row):\n",
    "    paragraphs_with_triplets = []\n",
    "    for p in row['paragraphs']:\n",
    "        p = deepcopy(p)\n",
    "        triplets_str = str(jerx_mapping[(row['id'], p['idx'])])\n",
    "        p['paragraph_text'] = '\\n'.join([p['paragraph_text'], \"# Entity-relation-entity triplets\", triplets_str])\n",
    "        paragraphs_with_triplets.append(p)\n",
    "    row['paragraphs'] = paragraphs_with_triplets\n",
    "    return row\n",
    "\n",
    "df_paragraph_triplets = df.apply(enhance_paragraphs, axis=1) \n",
    "df_paragraph_triplets.head()\n",
    "print(df_paragraph_triplets.iloc[0]['paragraphs'][2]['paragraph_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(1, N_RUNS + 1):\n",
    "    for qdecomp, benchmark in [(False, benchmark_single), (True, benchmark_multi)]:\n",
    "        for qa_technique, qa_func in [('standard', answer_question_standard)]:\n",
    "            for top_k in [3, 5, 10]:\n",
    "                for retriever_name, retriever in [('bm25', bm25_retrieval), ('semantic', semantic_retrieval)]:\n",
    "                    _, scores = benchmark(df_paragraph_triplets, qa_func, retriever, ignore_errors=True)\n",
    "                    results.append({**scores, \"retrieval\": retriever_name, \"top_k\": top_k, \"context\": \"paragraphs+triplets\", \"qa\": qa_technique, \"qdecomp\": qdecomp, \"run\": run})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_paragraphs(row):\n",
    "    paragraphs_with_triplets = []\n",
    "    for p in row['paragraphs']:\n",
    "        triplets_str = str(jerx_mapping[(row['id'], p['idx'])])\n",
    "        for triplet in triplets_str.splitlines():\n",
    "            p = deepcopy(p) \n",
    "            p['title'] = \"\"\n",
    "            p['paragraph_text'] = triplet.strip()\n",
    "            paragraphs_with_triplets.append(p)\n",
    "    row['paragraphs'] = paragraphs_with_triplets\n",
    "    return row\n",
    "\n",
    "df_only_triplets = df.apply(replace_paragraphs, axis=1) \n",
    "df_only_triplets.head()\n",
    "print(df_only_triplets.iloc[0]['paragraphs'][0]['paragraph_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(1, N_RUNS + 1):\n",
    "    for qdecomp, benchmark in [(False, benchmark_single), (True, benchmark_multi)]:\n",
    "        for qa_technique, qa_func in [('standard', answer_question_standard)]:\n",
    "            for top_k in [3, 5]:\n",
    "                top_k_effective = top_k*3 if qdecomp else top_k*5\n",
    "                for retriever_name, retriever in [\n",
    "                        ('bm25', partial(bm25_retrieval, top_k=top_k_effective)), \n",
    "                        ('semantic', partial(semantic_retrieval, top_k=top_k_effective)), \n",
    "                    ]:\n",
    "                    _, scores = benchmark(df_only_triplets, qa_func, retriever, ignore_errors=True)\n",
    "                    results.append({**scores, \"retrieval\": retriever_name, \"top_k\": top_k_effective, \"context\": \"triplets\", \"qa\": qa_technique, \"qdecomp\": qdecomp, \"run\": run})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.DataFrame.from_records(results, columns=['qdecomp', 'context', 'retrieval', 'top_k', 'qa', 'run', 'exact_match', 'f1'])\n",
    "report_df.sort_values('exact_match', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "suffix = datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "report_df.to_json(f'./our-method-report-{suffix}.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(report_df[report_df['context']=='paragraphs'].to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
