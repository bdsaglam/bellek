{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json('../../data/generated/musique-kg-llm/train/dataset.jsonl', orient=\"records\", lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import kuzu\n",
    "from llama_index import Document, KnowledgeGraphIndex, ServiceContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms import OpenAI\n",
    "from llama_index.prompts.base import Prompt\n",
    "from llama_index.prompts.prompt_type import PromptType\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from pyvis.network import Network\n",
    "from llama_index.callbacks import CallbackManager\n",
    "from bellek.ml.llm.obs import make_phoenix_trace_callback_handler\n",
    "\n",
    "from bellek.kuzu import KuzuGraphStore\n",
    "from bellek.jerx.utils import parse_triplets\n",
    "\n",
    "def _parse_triplet_response(response: str, max_length: int = 128) -> list[tuple[str, str, str]]:\n",
    "    return parse_triplets(response.strip())\n",
    "\n",
    "KnowledgeGraphIndex._parse_triplet_response = staticmethod(_parse_triplet_response)\n",
    "\n",
    "# Setup LLM observability\n",
    "LLM_TRACES_FILEPATH = Path(\"/tmp/phoenix/debug/traces.jsonl\")\n",
    "callback_manager = CallbackManager(handlers=[make_phoenix_trace_callback_handler(LLM_TRACES_FILEPATH)])\n",
    "\n",
    "def make_service_context(model_type: str):\n",
    "    if model_type == \"llama2\":\n",
    "        from bellek.llama_index.llms import HuggingFaceTextGenInferenceLLM\n",
    "\n",
    "        inference_server_url = \"http://localhost:8080/\"\n",
    "        llm = HuggingFaceTextGenInferenceLLM(\n",
    "            inference_server_url=inference_server_url,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=200,\n",
    "            repetition_penalty=1.0,\n",
    "            # top_k=50,\n",
    "            # top_p=1.0,\n",
    "            # typical_p=1.0,\n",
    "            # temperature=0.0,\n",
    "        )\n",
    "    else:\n",
    "        llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    # model to generate embeddings for triplets\n",
    "    embed_model = HuggingFaceEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    return ServiceContext.from_defaults(\n",
    "        llm=llm,\n",
    "        embed_model=embed_model,\n",
    "        callback_manager=callback_manager,\n",
    "    )\n",
    "\n",
    "\n",
    "LLAMA2_KG_TRIPLET_EXTRACT_TMPL = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a helpful assistant that extracts up to {max_knowledge_triplets}  entity-relation-entity triplets from given text. Use '|' as delimiter and provide one triplet per line.\n",
    "<</SYS>>\n",
    "Alaa Abdul Zahra plays for Al Shorta SC. His club is AL Kharaitiyat SC, which has its ground at, Al Khor. [/INST] Al Kharaitiyat SC|ground|Al Khor\n",
    "Alaa Abdul-Zahra|club|Al Kharaitiyat SC\n",
    "Alaa Abdul-Zahra|club|Al Shorta SC </s><s>[INST] {text} [/INST] \"\"\"\n",
    "\n",
    "DEFAULT_KG_TRIPLET_EXTRACT_TMPL = \"\"\"\n",
    "Some text is provided below. Given the text, extract up to {max_knowledge_triplets}  knowledge triplets in the form of (subject, predicate, object) that might be relevant to the following question. The subject and object must be different.\n",
    "Prioritize triplets that:\n",
    "1. Offer temporal information like 'founded in,' 'created on,' 'abolished in,' etc.\n",
    "2. Provide spatial details such as 'located in,' 'borders,' 'from,' etc.\n",
    "3. Show ownership or affiliation via terms like 'owned by,' 'affiliated with,' 'publisher of,' etc.\n",
    "4. Offer identification or categorization like 'is,' 'are,' 'was,' etc.\n",
    "Avoid stopwords.\n",
    "---------------------\n",
    "Example:\n",
    "Question: When was the institute that owned The Collegian founded?\n",
    "Text: The Collegian is the bi-weekly official student publication of Houston Baptist University in Houston, Texas.\n",
    "Triplets:\n",
    "(The Collegian, is, bi-weekly official student publication)\n",
    "(The Collegian, owned by, Houston Baptist University)\n",
    "(Houston Baptist University, in, Houston)\n",
    "(Houston, in, Texas)\n",
    "---------------------\n",
    "Text: {text}\n",
    "Triplets:\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def make_erx_prompt(model_type: str):\n",
    "    prompt_str = LLAMA2_KG_TRIPLET_EXTRACT_TMPL if model_type == \"llama2\" else DEFAULT_KG_TRIPLET_EXTRACT_TMPL\n",
    "    return Prompt(\n",
    "        prompt_str,\n",
    "        prompt_type=PromptType.KNOWLEDGE_TRIPLET_EXTRACT,\n",
    "    )\n",
    "\n",
    "\n",
    "def make_docs(example, only_supporting=False):\n",
    "    ps = example[\"paragraphs\"]\n",
    "    for p in ps:\n",
    "        if only_supporting and not p[\"is_supporting\"]:\n",
    "            continue\n",
    "        idx = p[\"idx\"]\n",
    "        title = p[\"title\"]\n",
    "        body = p[\"paragraph_text\"]\n",
    "        is_supporting = p[\"is_supporting\"]\n",
    "        text = f\"# {title}\\n{body}\"\n",
    "        yield Document(\n",
    "            text=text, \n",
    "            metadata=dict(parent_id=example[\"id\"], idx=idx, is_supporting=is_supporting),\n",
    "            excluded_llm_metadata_keys=[\"parent_id\", \"idx\", \"is_supporting\"],\n",
    "        )\n",
    "\n",
    "\n",
    "def construct_knowledge_graph(\n",
    "    example,\n",
    "    *,\n",
    "    max_triplets_per_chunk: int,\n",
    "    include_embeddings: bool,\n",
    "    model_type: str,\n",
    "    out_dir: Path,\n",
    "):\n",
    "    db = kuzu.Database(str(out_dir / \"kuzu\"))\n",
    "    graph_store = KuzuGraphStore(db)\n",
    "    storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "    # documents to index into knowledge graph\n",
    "    documents = list(make_docs(example, only_supporting=True))\n",
    "\n",
    "    # extract triplets from documents\n",
    "    return KnowledgeGraphIndex.from_documents(\n",
    "        documents=documents,\n",
    "        max_triplets_per_chunk=max_triplets_per_chunk,\n",
    "        storage_context=storage_context,\n",
    "        service_context=make_service_context(model_type),\n",
    "        include_embeddings=include_embeddings,\n",
    "        kg_triple_extract_template=make_erx_prompt(model_type),\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def visualize_knowledge_graph(index, out: Path):\n",
    "    g = index.get_networkx_graph()\n",
    "    net = Network(notebook=False, cdn_resources=\"in_line\", directed=True)\n",
    "    net.from_nx(g)\n",
    "    net.save_graph(str(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "example = df.iloc[i]\n",
    "id = example['id']\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = Path(\"../../tmp/kg-llama2\")\n",
    "\n",
    "example_out_dir = OUT / id\n",
    "shutil.rmtree(example_out_dir, ignore_errors=True)\n",
    "example_out_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = construct_knowledge_graph(\n",
    "    example,\n",
    "    max_triplets_per_chunk=10,\n",
    "    include_embeddings=True,\n",
    "    model_type=\"llama2\",\n",
    "    out_dir=example_out_dir,\n",
    ")\n",
    "index.storage_context.persist(persist_dir=(example_out_dir / \"index\"))\n",
    "print(f\"Visualizing the knowledge graph for the sample {id}\")\n",
    "visualize_knowledge_graph(index, example_out_dir / \"kuzu-network.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
