{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import magentic\n",
    "import logging\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import bm25s\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from bellek.musique.constants import ABLATION_RECORD_IDS\n",
    "from bellek.musique.singlehop import benchmark as benchmark_single\n",
    "from bellek.musique.multihop import benchmark as benchmark_multi\n",
    "from bellek.musique.qa import answer_question_standard\n",
    "from bellek.utils import set_seed\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "tqdm.pandas()\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "\n",
    "set_seed(89)\n",
    "\n",
    "logging.getLogger(\"bm25s\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pc/anaconda3/envs/baris/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval functions\n",
    "def bm25_retrieval(docs: list[dict], query: str, top_k: int):\n",
    "    top_k = min(top_k, len(docs))\n",
    "    retriever = bm25s.BM25(corpus=docs)\n",
    "    tokenized_corpus = bm25s.tokenize([doc[\"text\"] for doc in docs], show_progress=False)\n",
    "    retriever.index(tokenized_corpus, show_progress=False)\n",
    "    results, _ = retriever.retrieve(bm25s.tokenize(query), k=top_k, show_progress=False)\n",
    "    return results[0].tolist()\n",
    "\n",
    "\n",
    "def semantic_retrieval(docs: list[dict], query: str, top_k: int):\n",
    "    embeddings = model.encode([doc[\"text\"] for doc in docs])\n",
    "    query_vectors = model.encode([query])\n",
    "    similarities = model.similarity(embeddings, query_vectors)\n",
    "    sorted_indices = similarities.argsort(dim=0, descending=True)\n",
    "    return [docs[i] for i in sorted_indices[:top_k]]\n",
    "\n",
    "\n",
    "def dummy_retrieval(docs: list[dict], query: str, top_k: int):\n",
    "    return docs\n",
    "\n",
    "\n",
    "def perfect_retrieval(docs: list[dict], query: str, top_k: int):\n",
    "    return [doc for doc in docs if doc[\"is_supporting\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../../data/generated/musique-evaluation/dataset.jsonl\", orient=\"records\", lines=True)\n",
    "df = df.set_index(\"id\", drop=False).loc[ABLATION_RECORD_IDS].copy().reset_index(drop=True)\n",
    "\n",
    "\n",
    "qd_df = pd.read_json(\n",
    "    \"../../data/generated/musique-evaluation/question-decomposition.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    ")\n",
    "df = pd.merge(df.drop(columns=[\"question\", \"question_decomposition\"]), qd_df, on=\"id\", suffixes=(\"\", \"\"))\n",
    "\n",
    "\n",
    "jerx_file = Path(\"../../data/raw/musique-evaluation/jerx-inferences/llama3-base.jsonl\")\n",
    "jerx_df = pd.read_json(jerx_file, lines=True)\n",
    "\n",
    "jerx_mapping = {(row[\"id\"], row[\"paragraph_idx\"]): row[\"generation\"] for _, row in jerx_df.iterrows()}\n",
    "\n",
    "\n",
    "def extract_triplets(example: dict):\n",
    "    example[\"triplets_str\"] = [jerx_mapping[(example[\"id\"], p[\"idx\"])].strip() for p in example[\"paragraphs\"]]\n",
    "    return example\n",
    "\n",
    "\n",
    "df = df.apply(extract_triplets, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running QA experiments with only triplets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7296877c590e4089bc70d9102089b1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "suffix = datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "RESULTS_FILE = Path(f\"./our-method-kgqa-results-{suffix}.jsonl\")\n",
    "REPORT_FILE = Path(f\"./our-method-kgqa-report-{suffix}.jsonl\")\n",
    "\n",
    "N_RUNS = 1\n",
    "\n",
    "results = []\n",
    "\n",
    "# Parameters\n",
    "qa_retry_deco = retry(stop=stop_after_attempt(3), wait=wait_random_exponential(multiplier=1, max=30))\n",
    "llm = magentic.OpenaiChatModel(\"gpt-3.5-turbo\", temperature=0.1)\n",
    "\n",
    "# Hyperparamaters\n",
    "qdecomp_params = [\n",
    "    (False, benchmark_single),\n",
    "    # (True, benchmark_multi),\n",
    "]\n",
    "\n",
    "prompting_params = [\n",
    "    (\"Standard\", answer_question_standard),\n",
    "]\n",
    "\n",
    "retrieval_params = [\n",
    "    # (\"Sparse\", bm25_retrieval, [5, 10, 15, 20, 30, 40, 50, 70]),\n",
    "    # (\"Dense\", semantic_retrieval, [5, 10, 15, 20, 30, 40, 50, 70]),\n",
    "    # (\"Dummy\", dummy_retrieval, [20]),\n",
    "    (\"Perfect\", perfect_retrieval, [2]),\n",
    "]\n",
    "\n",
    "# ## Only triplets\n",
    "\n",
    "print(\"Running QA experiments with only triplets\")\n",
    "\n",
    "def replace_paragraphs_with_triplets(row):\n",
    "    paragraphs_with_triplets = []\n",
    "    for p in row[\"paragraphs\"]:\n",
    "        triplets_str = str(jerx_mapping[(row[\"id\"], p[\"idx\"])])\n",
    "        for triplet in triplets_str.splitlines():\n",
    "            p = deepcopy(p)\n",
    "            p[\"title\"] = \"\"\n",
    "            p[\"paragraph_text\"] = triplet.strip()\n",
    "            paragraphs_with_triplets.append(p)\n",
    "    row[\"paragraphs\"] = paragraphs_with_triplets\n",
    "    return row\n",
    "\n",
    "\n",
    "df = df.apply(replace_paragraphs_with_triplets, axis=1)\n",
    "\n",
    "dfs = []\n",
    "\n",
    "with llm:\n",
    "    for run in range(1, N_RUNS + 1):\n",
    "        for qdecomp, benchmark in qdecomp_params:\n",
    "            for qa_technique, qa_func in prompting_params:\n",
    "                for retriever_name, retriever, top_ks in retrieval_params:\n",
    "                    for top_k in top_ks:\n",
    "                        df_result, scores = benchmark(\n",
    "                            df,\n",
    "                            qa_retry_deco(qa_func),\n",
    "                            partial(retriever, top_k=top_k),\n",
    "                        )\n",
    "                        dfs.append(df_result)\n",
    "                        results.append(\n",
    "                            {\n",
    "                                **scores,\n",
    "                                \"qdecomp\": qdecomp,\n",
    "                                \"context\": \"Triplets\",\n",
    "                                \"retrieval\": retriever_name,\n",
    "                                \"top_k\": top_k,\n",
    "                                \"qa\": qa_technique,\n",
    "                                \"run\": run,\n",
    "                            }\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q&A results\n",
    "dfs[0].to_json(RESULTS_FILE, orient=\"records\", lines=True)\n",
    "\n",
    "# Reporting scores\n",
    "report_df = pd.DataFrame.from_records(\n",
    "    results,\n",
    "    columns=[\"qdecomp\", \"context\", \"retrieval\", \"top_k\", \"qa\", \"run\", \"exact_match\", \"f1\"],\n",
    ")\n",
    "report_df.to_json(REPORT_FILE, orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(dfs) == 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
