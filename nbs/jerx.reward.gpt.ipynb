{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward model JERX task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp jerx.reward.gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains.openai_functions import create_structured_output_runnable\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from bellek.text.utils import fuzzy_match\n",
    "from bellek.logging import get_logger\n",
    "\n",
    "log = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are an expert Q&A system that is trusted around the world. You are given a question that requires multi-hop reasoning. Always answer the question using the provided context information, and not prior knowledge.\n",
    "Some rules to follow:\n",
    "1. Never directly reference the given context in your answer.\n",
    "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"The context information below is provided as a set of entity-relation-entity triplets from knowledge graph.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the question.\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "DEFAULT_PROMPT_MESSAGES = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", DEFAULT_SYSTEM_PROMPT), \n",
    "    (\"user\", USER_PROMPT),\n",
    "])\n",
    "\n",
    "\n",
    "class QuestionAnsweringResult(BaseModel):\n",
    "    \"\"\"Data model for answering the question.\"\"\"\n",
    "    answer: str = Field(description=\"The answer to the question in 2-4 words.\")\n",
    "    reasoning: str = Field(description=\"Multi-hop reasoning for the answer.\")\n",
    "\n",
    "\n",
    "def make_question_answer_func(llm: ChatOpenAI | None = None):\n",
    "    if llm is None:\n",
    "        llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\", temperature=0)\n",
    "\n",
    "    chain = create_structured_output_runnable(QuestionAnsweringResult, llm=llm, prompt=DEFAULT_PROMPT_MESSAGES)\n",
    "    def func(context: str, question: str) -> QuestionAnsweringResult:\n",
    "        return chain.invoke(dict(context=context, question=question))\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class RewardAssessment(QuestionAnsweringResult):\n",
    "    reward: float = Field(description=\"The reward value for the answer.\")\n",
    "\n",
    "def make_reward_func(llm: ChatOpenAI | None = None, answer_comparator=fuzzy_match):\n",
    "    qa = make_question_answer_func(llm)\n",
    "\n",
    "    def reward(context: str, question: str, answers: list[str]) -> RewardAssessment:\n",
    "        qa_result = qa(context, question)\n",
    "        correct = any(answer_comparator(qa_result.answer, answer) for answer in answers)\n",
    "        reward = 1.0 if correct else 0.0\n",
    "        return RewardAssessment(**qa_result.dict(), reward=reward)\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringResult(answer='1996', reasoning='Dominica first competed at Olympic Games in 1996.')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets = [ \"Dominica | first competed at | Olympic Games in 1996\", \"Dominica | has participated in | each Games since then\", \"Dominica | has not won | any medals at the Olympic Games\" ]\n",
    "\n",
    "context = \"\\n\".join(triplets)\n",
    "question = \"When did the country where the River Quanery  is found first compete in Olympic games?\"\n",
    "answer = \"1996\"\n",
    "\n",
    "qa = make_question_answer_func()\n",
    "qa(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_func = make_reward_func()\n",
    "assert reward_func(context, question, [answer]).reward == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_match('Marta Hernández Romero', 'Marta Hernández Romero')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
