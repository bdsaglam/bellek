{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers utils for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp hf.transformers.experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "from copy import deepcopy\n",
    "from math import ceil\n",
    "from typing import Any, Callable\n",
    "\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import TrainingArguments, pipeline\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTTrainer\n",
    "\n",
    "from bellek.hf.transformers.utils import load_tokenizer_model\n",
    "from bellek.logging import get_logger\n",
    "from bellek.utils import NestedDict, generate_time_id\n",
    "\n",
    "log = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def preprocess_config(config: NestedDict):\n",
    "    config = deepcopy(config)\n",
    "\n",
    "    # Set float precision\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        log.info(\"GPU supports bfloat16.\")\n",
    "        torch_dtype, bf16, fp16, bnb_4bit_compute_dtype = (\"bfloat16\", True, False, \"bfloat16\")\n",
    "    else:\n",
    "        log.info(\"GPU does not support bfloat16.\")\n",
    "        torch_dtype, bf16, fp16, bnb_4bit_compute_dtype = (\"float16\", False, True, \"float16\")\n",
    "\n",
    "    if config.at(\"pretrained_model.torch_dtype\"):\n",
    "        config.set(\"pretrained_model.torch_dtype\", torch_dtype)\n",
    "    if config.at(\"pretrained_model.quantization_config.load_in_4bit\"):\n",
    "        config.set(\"pretrained_model.quantization_config.bnb_4bit_compute_dtype\", bnb_4bit_compute_dtype)\n",
    "    if config.at(\"trainer.training_args.bf16\") or config.at(\"trainer.training_args.fp16\"):\n",
    "        config.set(\"trainer.training_args.bf16\", bf16)\n",
    "        config.set(\"trainer.training_args.fp16\", fp16)\n",
    "    \n",
    "    # Generate unique model id\n",
    "    model_id = config.at(\"hfhub.model_id\")\n",
    "    if config.at(\"trainer.lora\") and \"-peft\" not in model_id:\n",
    "        model_id += \"-peft\"\n",
    "    if \"debug\" not in model_id:\n",
    "        model_id += f\"-{generate_time_id()}\"\n",
    "    config.set(\"hfhub.model_id\", model_id)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def make_datacollator(tokenizer, response_template: str | None, response_template_context: str | None = None):\n",
    "    if not response_template:\n",
    "        return None\n",
    "\n",
    "    if response_template_context is None:\n",
    "        log.info(f\"Creating completion-only data collator with response template '{response_template}'\")\n",
    "        data_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "    else:\n",
    "        log.info(f\"Creating completion-only data collator with response template '{response_template}' and context '{response_template_context}'\")\n",
    "        response_template_with_context = response_template_context + response_template\n",
    "        response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[len(response_template_context):] \n",
    "        data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n",
    "    \n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def fine_tune(config: NestedDict):\n",
    "    from peft import LoraConfig\n",
    "\n",
    "    # Base model\n",
    "    pretrained_model_config = config[\"pretrained_model\"]\n",
    "    model_id = pretrained_model_config.pop(\"model_name_or_path\")\n",
    "    tokenizer, base_model = load_tokenizer_model(model_id, **pretrained_model_config)\n",
    "    log.info(f\"Loaded base model {model_id}\")\n",
    "\n",
    "    if \"llama\" in model_id:\n",
    "        from bellek.hf.transformers.llama import prepare_llama2_for_training\n",
    "\n",
    "        log.info(\"Base model is a LLAMA model, preparing it for training.\")\n",
    "        prepare_llama2_for_training(tokenizer, base_model)\n",
    "\n",
    "    # Train dataset\n",
    "    train_ds = load_dataset(**config.at(\"dataset.train\"))\n",
    "    log.info(f\"Loaded training dataset with {len(train_ds)} samples.\")\n",
    "\n",
    "    # Inspect token counts\n",
    "    tokenized_train_ds = train_ds.map(lambda examples: tokenizer(examples[\"text\"]), batched=True)\n",
    "    token_counts = [len(input_ids) for input_ids in tokenized_train_ds[\"input_ids\"]]\n",
    "    log.info(f\"Input token counts: min={min(token_counts)}, max={max(token_counts)}\")\n",
    "\n",
    "    # Supervised fine-tuning\n",
    "    if config.at(\"trainer.max_seq_length\") is None:\n",
    "        config.set(\"trainer.max_seq_length\", ceil(max(token_counts) / 8) * 8)\n",
    "    max_seq_length = config.at(\"trainer.max_seq_length\")\n",
    "    log.info(f\"Setting max_seq_length={max_seq_length}\")\n",
    "\n",
    "    packing = config.at(\"trainer.packing\", False)\n",
    "\n",
    "    data_collator=make_datacollator(\n",
    "        tokenizer, \n",
    "        config.at(\"trainer.response_template\"), \n",
    "        config.at(\"trainer.response_template_context\")\n",
    "    )\n",
    "    \n",
    "    peft_config = LoraConfig(**config.at(\"trainer.lora\", {}))\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        **config.at(\"trainer.training_args\"),\n",
    "    )\n",
    "    trainer = SFTTrainer(\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        peft_config=peft_config,\n",
    "        train_dataset=train_ds,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=packing,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "    )\n",
    "    log.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model\n",
    "    log.info(\"Saving model...\")\n",
    "    final_model_id = config.at(\"hfhub.model_id\")\n",
    "    trainer.model.push_to_hub(final_model_id)\n",
    "    tokenizer.push_to_hub(final_model_id)\n",
    "    log.info(f\"Uploaded PEFT adapters to HF Hub as {final_model_id}\")\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def make_io_dataset(dataset: Dataset, response_template: str) -> Dataset:\n",
    "    def extract_input_output(example):\n",
    "        input, output = example[\"text\"].rsplit(response_template, 1)\n",
    "        input += response_template\n",
    "        return {\"input\": input, \"output\": output}\n",
    "\n",
    "    return dataset.map(extract_input_output)\n",
    "\n",
    "\n",
    "def _load_tokenizer_model(config: NestedDict):\n",
    "    model_id = config.at(\"hfhub.model_id\")\n",
    "    quantization_config = config.at(\"pretrained_model.quantization_config\")\n",
    "    return load_tokenizer_model(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "\n",
    "def make_pipeline(config, tokenizer, model):\n",
    "    model_id = config.at(\"hfhub.model_id\")\n",
    "    if \"llama\" in model_id:\n",
    "        from bellek.hf.transformers.llama import prepare_llama2_for_inference\n",
    "\n",
    "        prepare_llama2_for_inference(tokenizer, model)\n",
    "\n",
    "    # Create pipeline\n",
    "    return pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        **config.at(\"evaluation.pipeline\", {}),\n",
    "    )\n",
    "\n",
    "\n",
    "def flat_pipeline(pipe):\n",
    "    def func(inputs) -> list[str]:\n",
    "        results = pipe(inputs)\n",
    "        return [result[0][\"generated_text\"] for result in results]\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def evaluate_pipeline(\n",
    "    dataset,\n",
    "    pipe,\n",
    "    *,\n",
    "    metric,\n",
    "    metric_kwargs: dict | None = None,\n",
    "    output_parse_fn: Callable[[str], Any] | None = None,\n",
    "):\n",
    "    eos_token = pipe.tokenizer.special_tokens_map[\"eos_token\"]\n",
    "    def parse_output(text):\n",
    "        text = text.replace(eos_token, \"\").strip()\n",
    "        if output_parse_fn:\n",
    "            text = output_parse_fn(text)\n",
    "        return text\n",
    "\n",
    "    log.info(f\"Running pipeline on dataset with {len(dataset)} samples...\")\n",
    "    generations = flat_pipeline(pipe)(dataset[\"input\"])\n",
    "\n",
    "    predictions = [parse_output(text) for text in generations]\n",
    "    references = [parse_output(text) for text in dataset[\"output\"]]\n",
    "\n",
    "    dataf = dataset.to_pandas()\n",
    "    dataf[\"generation\"] = generations\n",
    "    dataf[\"prediction\"] = predictions\n",
    "    dataf[\"reference\"] = references\n",
    "    \n",
    "    metric_kwargs = metric_kwargs or {}\n",
    "    scores = metric.compute(predictions=predictions, references=references, **metric_kwargs)\n",
    "\n",
    "    return scores, dataf\n",
    "\n",
    "\n",
    "def evalu8(\n",
    "    config,\n",
    "    *,\n",
    "    tokenizer=None,\n",
    "    model=None,\n",
    "    metric_kwargs: dict | None = None,\n",
    "    output_parse_fn: Callable[[str], Any] | None = None,\n",
    "):\n",
    "    import evaluate\n",
    "\n",
    "    # Load validation dataset\n",
    "    ds_config = config.at(\"dataset.validation\")\n",
    "    assert ds_config\n",
    "    ds = load_dataset(**ds_config)\n",
    "    assert len(ds) > 0, \"Dataset is empty!\"\n",
    "\n",
    "    # Ensure the dataset has input/output columns\n",
    "    cols = ds[0].keys()\n",
    "    if \"input\" not in cols or \"output\" not in cols:\n",
    "        response_template = config.at(\"trainer.response_template\")\n",
    "        assert response_template\n",
    "        ds = make_io_dataset(ds, response_template)\n",
    "\n",
    "    # Prepare text generation pipeline\n",
    "    if tokenizer is None or model is None:\n",
    "        tokenizer, model = _load_tokenizer_model(config)\n",
    "    \n",
    "    if config.at(\"evaluation.pipeline.max_new_tokens\") is None:\n",
    "        tokenized_outputs = ds.map(lambda examples: tokenizer(examples[\"output\"]), batched=True)\n",
    "        token_counts = [len(input_ids) for input_ids in tokenized_outputs[\"input_ids\"]]\n",
    "        config.set(\"evaluation.pipeline.max_new_tokens\", ceil(max(token_counts) / 8) * 8)\n",
    "        \n",
    "    log.info(f\"Input token counts: min={min(token_counts)}, max={max(token_counts)}\")\n",
    "    pipe = make_pipeline(config, tokenizer, model)\n",
    "\n",
    "    # Load evaluation metric\n",
    "    metric = evaluate.load(config.at(\"evaluation.metric\"))\n",
    "\n",
    "    return evaluate_pipeline(\n",
    "        ds,\n",
    "        pipe,\n",
    "        metric=metric,\n",
    "        metric_kwargs=metric_kwargs,\n",
    "        output_parse_fn=output_parse_fn,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
