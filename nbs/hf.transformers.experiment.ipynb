{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers utils for experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp hf.transformers.experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "from copy import deepcopy\n",
    "from math import ceil\n",
    "from typing import Any, Callable\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, pipeline\n",
    "from trl import DataCollatorForCompletionOnlyLM, SFTTrainer\n",
    "\n",
    "from bellek.hf.transformers.utils import load_tokenizer_model\n",
    "from bellek.hf.datasets.utils import load_datasets\n",
    "from bellek.logging import get_logger\n",
    "from bellek.utils import NestedDict, generate_time_id\n",
    "\n",
    "log = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def prepare_config_for_fp(config: NestedDict):\n",
    "    if not torch.cuda.is_available():\n",
    "        return config\n",
    "\n",
    "    # Set float precision\n",
    "    if config.at(\"pretrained_model.torch_dtype\") in {\"float16\", \"bfloat16\"}:\n",
    "        major, _ = torch.cuda.get_device_capability()\n",
    "        gpu_supports_bf = major >= 8\n",
    "        if gpu_supports_bf:\n",
    "            log.info(\"GPU supports bfloat16.\")\n",
    "        else:\n",
    "            log.info(\"GPU does not support bfloat16.\")\n",
    "        \n",
    "        if config.at(\"pretrained_model.torch_dtype\") == \"bfloat16\" and gpu_supports_bf:\n",
    "            log.info(\"Using bfloat16.\")\n",
    "            torch_dtype, bf16, fp16, bnb_4bit_compute_dtype = (\"bfloat16\", True, False, \"bfloat16\")\n",
    "        else:\n",
    "            log.info(\"Using float16.\")\n",
    "            torch_dtype, bf16, fp16, bnb_4bit_compute_dtype = (\"float16\", False, True, \"float16\")\n",
    "    else:\n",
    "        log.info(\"Not using half-precision float.\")\n",
    "        torch_dtype, bf16, fp16, bnb_4bit_compute_dtype = (None, None, None, None)\n",
    "\n",
    "    if config.at(\"pretrained_model.torch_dtype\"):\n",
    "        config.set(\"pretrained_model.torch_dtype\", torch_dtype)\n",
    "    if config.at(\"pretrained_model.quantization_config.load_in_4bit\"):\n",
    "        config.set(\"pretrained_model.quantization_config.bnb_4bit_compute_dtype\", bnb_4bit_compute_dtype)\n",
    "    if config.at(\"trainer.training_args.bf16\") or config.at(\"trainer.training_args.fp16\"):\n",
    "        config.set(\"trainer.training_args.bf16\", bf16)\n",
    "        config.set(\"trainer.training_args.fp16\", fp16)\n",
    "    \n",
    "    return config\n",
    "\n",
    "def preprocess_config(config: NestedDict):\n",
    "    config = deepcopy(config)\n",
    "\n",
    "    if isinstance(config.at(\"dataset.train\"), dict):\n",
    "        config.set(\"dataset.train\", [config.at(\"dataset.train\")])\n",
    "    if isinstance(config.at(\"dataset.validation\"), dict):\n",
    "        config.set(\"dataset.validation\", [config.at(\"dataset.validation\")])\n",
    "\n",
    "    config = prepare_config_for_fp(config)\n",
    "    \n",
    "    # Generate unique model id for output model\n",
    "    if (out_model_id := config.at(\"hfhub.model_id\")) and config.at(\"metaconfig.preprocessing.unique_hfhub_model_id\"):\n",
    "        if \"-peft\" not in out_model_id and config.at(\"trainer.lora\"):\n",
    "            out_model_id += \"-peft\"\n",
    "        if wandb_run_id := config.at(\"wandb.run_id\"):\n",
    "            out_model_id += f\"-{wandb_run_id}\"\n",
    "        else:\n",
    "            out_model_id += f\"-{generate_time_id()}\"\n",
    "        config.set(\"hfhub.model_id\", out_model_id)\n",
    "\n",
    "    return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 42,\n",
       " 'dataset': {'train': [{'path': 'bdsaglam/webnlg-jerx-sft-multi-turn-multi-sentence-openai',\n",
       "    'split': 'train'}],\n",
       "  'validation': [{'path': 'bdsaglam/webnlg-jerx-sft-multi-turn-multi-sentence-openai',\n",
       "    'split': 'dev[:256]'}]},\n",
       " 'pretrained_model': {'model_name_or_path': 'NousResearch/Llama-2-7b-chat-hf',\n",
       "  'torch_dtype': 'bfloat16',\n",
       "  'quantization_config': {'load_in_8bit': False,\n",
       "   'load_in_4bit': True,\n",
       "   'bnb_4bit_quant_type': 'nf4'}},\n",
       " 'trainer': {'packing': False,\n",
       "  'lora': {'lora_alpha': 16,\n",
       "   'lora_dropout': 0.1,\n",
       "   'r': 64,\n",
       "   'bias': 'none',\n",
       "   'task_type': 'CAUSAL_LM'},\n",
       "  'response_template': '[/INST]',\n",
       "  'response_template_context': ' ',\n",
       "  'training_args': {'bf16': True,\n",
       "   'fp16': False,\n",
       "   'group_by_length': True,\n",
       "   'per_device_train_batch_size': 4,\n",
       "   'gradient_accumulation_steps': 2,\n",
       "   'gradient_checkpointing': True,\n",
       "   'max_grad_norm': 0.3,\n",
       "   'weight_decay': 0.001,\n",
       "   'learning_rate': 0.0002,\n",
       "   'lr_scheduler_type': 'cosine',\n",
       "   'warmup_ratio': 0.03,\n",
       "   'optim': 'paged_adamw_32bit',\n",
       "   'max_steps': -1,\n",
       "   'num_train_epochs': 1,\n",
       "   'logging_steps': 25,\n",
       "   'save_steps': 0,\n",
       "   'report_to': 'wandb'}},\n",
       " 'evaluation': {'pipeline': {'batch_size': 8, 'return_full_text': False},\n",
       "  'metric': 'bdsaglam/jer'},\n",
       " 'wandb': {'mode': 'online', 'entity': 'bdsaglam', 'project': 'thesis-kgcons'},\n",
       " 'hfhub': {'model_id': 'bdsaglam/llama-2-7b-chat-jerx-peft-2024-04-20T12-40-15'},\n",
       " 'metaconfig': {'preprocessing': {'resolve_paths': False,\n",
       "   'unique_hfhub_model_id': True}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "\n",
    "import json\n",
    "config = json.loads(\"\"\"\n",
    "{\n",
    "  \"seed\": 42,\n",
    "  \"dataset\": {\n",
    "    \"train\": {\n",
    "      \"path\": \"bdsaglam/webnlg-jerx-sft-multi-turn-multi-sentence-openai\",\n",
    "      \"split\": \"train\"\n",
    "    },\n",
    "    \"validation\": {\n",
    "      \"path\": \"bdsaglam/webnlg-jerx-sft-multi-turn-multi-sentence-openai\",\n",
    "      \"split\": \"dev[:256]\"\n",
    "    }\n",
    "  },\n",
    "  \"pretrained_model\": {\n",
    "    \"model_name_or_path\": \"meta-llama/llama-2-7b-chat-hf\",\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"quantization_config\": {\n",
    "      \"load_in_8bit\": false,\n",
    "      \"load_in_4bit\": true,\n",
    "      \"bnb_4bit_quant_type\": \"nf4\"\n",
    "    }\n",
    "  },\n",
    "  \"trainer\": {\n",
    "    \"packing\": false,\n",
    "    \"lora\": {\n",
    "      \"lora_alpha\": 16,\n",
    "      \"lora_dropout\": 0.1,\n",
    "      \"r\": 64,\n",
    "      \"bias\": \"none\",\n",
    "      \"task_type\": \"CAUSAL_LM\"\n",
    "    },\n",
    "    \"response_template\": \"[/INST]\",\n",
    "    \"response_template_context\": \" \",\n",
    "    \"training_args\": {\n",
    "      \"bf16\": true,\n",
    "      \"fp16\": false,\n",
    "      \"group_by_length\": true,\n",
    "      \"per_device_train_batch_size\": 4,\n",
    "      \"gradient_accumulation_steps\": 2,\n",
    "      \"gradient_checkpointing\": true,\n",
    "      \"max_grad_norm\": 0.3,\n",
    "      \"weight_decay\": 0.001,\n",
    "      \"learning_rate\": 0.0002,\n",
    "      \"lr_scheduler_type\": \"cosine\",\n",
    "      \"warmup_ratio\": 0.03,\n",
    "      \"optim\": \"paged_adamw_32bit\",\n",
    "      \"max_steps\": -1,\n",
    "      \"num_train_epochs\": 1,\n",
    "      \"logging_steps\": 25,\n",
    "      \"save_steps\": 0,\n",
    "      \"report_to\": \"wandb\"\n",
    "    }\n",
    "  },\n",
    "  \"evaluation\": {\n",
    "    \"pipeline\": {\n",
    "      \"batch_size\": 8\n",
    "    },\n",
    "    \"metric\": \"bdsaglam/jer\"\n",
    "  },\n",
    "  \"wandb\": {\n",
    "    \"mode\": \"online\",\n",
    "    \"entity\": \"bdsaglam\",\n",
    "    \"project\": \"thesis-kgcons\"\n",
    "  },\n",
    "  \"hfhub\": {\n",
    "    \"model_id\": \"bdsaglam/llama-2-7b-chat-jerx\"\n",
    "  },\n",
    "  \"metaconfig\": {\n",
    "    \"preprocessing\": {\n",
    "      \"resolve_paths\": false,\n",
    "      \"unique_hfhub_model_id\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "\n",
    "preprocess_config(NestedDict(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def make_datacollator(tokenizer, response_template: str | None, response_template_context: str | None = None):\n",
    "    if not response_template:\n",
    "        return None\n",
    "\n",
    "    if response_template_context is None:\n",
    "        log.info(f\"Creating completion-only data collator with response template '{response_template}'\")\n",
    "        data_collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "    else:\n",
    "        log.info(f\"Creating completion-only data collator with response template '{response_template}' and context '{response_template_context}'\")\n",
    "        response_template_with_context = response_template_context + response_template\n",
    "        response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[len(response_template_context):] \n",
    "        data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n",
    "    \n",
    "    return data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def prepare_model_for_training(tokenizer, model):\n",
    "    model_id = model.name_or_path.lower()\n",
    "\n",
    "    if \"llama-2\" in model_id:\n",
    "        from bellek.hf.transformers.llama2 import prepare_llama2_for_training\n",
    "\n",
    "        log.info(\"Base model is a llama-2 model, preparing it for training.\")\n",
    "        prepare_llama2_for_training(tokenizer, model)\n",
    "    \n",
    "    elif \"llama-3\" in model_id:\n",
    "        from bellek.hf.transformers.llama3 import prepare_llama3_for_training\n",
    "\n",
    "        log.info(\"Base model is a llama-3 model, preparing it for training.\")\n",
    "        prepare_llama3_for_training(tokenizer, model)\n",
    "    \n",
    "    else:\n",
    "        log.warning(f\"Base model '{model_id}' is not a llama-2 or llama-3 model, no special preparation is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def calculate_token_counts(tokenizer, dataset: Dataset, dataset_text_field: str | None):\n",
    "    if dataset_text_field is None:\n",
    "        if \"messages\" not in dataset.column_names:\n",
    "            raise ValueError(\"Dataset must have 'messages' columns if `dataset_text_field` is not specified.\")\n",
    "        \n",
    "        dataset_text_field = \"text\"\n",
    "        dataset = dataset.map(\n",
    "            lambda example: {dataset_text_field: tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False)}\n",
    "        )\n",
    "    \n",
    "    # Inspect token counts\n",
    "    tokenized_train_ds = dataset.map(lambda examples: tokenizer(examples[dataset_text_field]), batched=True)\n",
    "    token_counts = [len(input_ids) for input_ids in tokenized_train_ds[\"input_ids\"]]\n",
    "    log.info(f\"Input token counts: min={min(token_counts)}, max={max(token_counts)}\")\n",
    "    return token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def fine_tune(config: NestedDict):\n",
    "    from peft import LoraConfig\n",
    "\n",
    "    # Base model\n",
    "    pretrained_model_config = config[\"pretrained_model\"]\n",
    "    model_id = pretrained_model_config.pop(\"model_name_or_path\")\n",
    "    tokenizer, base_model = load_tokenizer_model(model_id, **pretrained_model_config)\n",
    "    log.info(f\"Loaded base model {model_id}\")\n",
    "\n",
    "    # Prepare model for training\n",
    "    prepare_model_for_training(tokenizer, base_model)\n",
    "\n",
    "    # Train dataset\n",
    "    train_ds = load_datasets(config.at(\"dataset.train\")).shuffle(seed=config.at(\"seed\"))\n",
    "    log.info(f\"Loaded training dataset with {len(train_ds)} samples.\")\n",
    "\n",
    "    # Inspect token counts\n",
    "    dataset_text_field = config.at(\"trainer.dataset_text_field\")\n",
    "    token_counts = calculate_token_counts(tokenizer, train_ds, dataset_text_field)\n",
    "    log.info(f\"Input token counts: min={min(token_counts)}, max={max(token_counts)}\")\n",
    "\n",
    "    # Supervised fine-tuning\n",
    "    if config.at(\"trainer.max_seq_length\") is None:\n",
    "        config.set(\"trainer.max_seq_length\", ceil(max(token_counts) / 8) * 8)\n",
    "    max_seq_length = config.at(\"trainer.max_seq_length\")\n",
    "    log.info(f\"Setting max_seq_length={max_seq_length}\")\n",
    "\n",
    "    peft_config = LoraConfig(**config.at(\"trainer.lora\", {}))\n",
    "\n",
    "    packing = config.at(\"trainer.packing\", False)\n",
    "\n",
    "    data_collator = make_datacollator(\n",
    "        tokenizer,\n",
    "        config.at(\"trainer.response_template\"),\n",
    "        config.at(\"trainer.response_template_context\"),\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        **config.at(\"trainer.training_args\"),\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        tokenizer=tokenizer,\n",
    "        model=base_model,\n",
    "        peft_config=peft_config,\n",
    "        train_dataset=train_ds,\n",
    "        dataset_text_field=dataset_text_field,\n",
    "        max_seq_length=max_seq_length,\n",
    "        packing=packing,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "    )\n",
    "    log.info(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model\n",
    "    log.info(\"Saving model...\")\n",
    "    final_model_id = config.at(\"hfhub.model_id\")\n",
    "    trainer.model.push_to_hub(final_model_id)\n",
    "    tokenizer.push_to_hub(final_model_id)\n",
    "    log.info(f\"Uploaded PEFT adapters to HF Hub as {final_model_id}\")\n",
    "\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def prepare_model_for_inference(tokenizer, model):\n",
    "    model_id = model.name_or_path.lower()\n",
    "\n",
    "    if \"llama-2\" in model_id:\n",
    "        from bellek.hf.transformers.llama2 import prepare_llama2_for_inference\n",
    "\n",
    "        log.info(\"Base model is a llama-2 model, preparing it for inference.\")\n",
    "        prepare_llama2_for_inference(tokenizer, model)\n",
    "    \n",
    "    elif \"llama-3\" in model_id:\n",
    "        from bellek.hf.transformers.llama3 import prepare_llama3_for_inference\n",
    "\n",
    "        log.info(\"Base model is a llama-3 model, preparing it for inference.\")\n",
    "        prepare_llama3_for_inference(tokenizer, model)\n",
    "    \n",
    "    else:\n",
    "        log.warning(f\"Base model '{model_id}' is not a llama-2 or llama-3 model, no special preparation is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def _load_tokenizer_model(config: NestedDict):\n",
    "    model_id = config.at(\"hfhub.model_id\")\n",
    "    kwargs = deepcopy(config.get(\"pretrained_model\", {}))\n",
    "    kwargs.pop(\"model_name_or_path\", None)\n",
    "    return load_tokenizer_model(model_id, **kwargs)\n",
    "\n",
    "\n",
    "def make_pipeline(config, tokenizer, model):\n",
    "    prepare_model_for_inference(tokenizer, model)\n",
    "    return pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        **config.at(\"evaluation.pipeline\", {}),\n",
    "    )\n",
    "\n",
    "\n",
    "def flat_pipeline(pipe):\n",
    "    def func(inputs, **kwargs) -> list[str]:\n",
    "        return [result[0][\"generated_text\"] for result in tqdm(pipe(inputs, **kwargs))]\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def predict(\n",
    "    pipe,\n",
    "    dataset,\n",
    "    *,\n",
    "    output_parse_fn: Callable[[str], Any] | None = None,\n",
    "    **generation_kwargs,\n",
    "):\n",
    "    output_parse_fn = output_parse_fn or (lambda x: x)\n",
    "\n",
    "    # Setup generation parameters\n",
    "    generation_kwargs[\"return_full_text\"] = False\n",
    "\n",
    "    if \"max_new_tokens\" not in generation_kwargs:\n",
    "        tokenized_outputs = dataset.map(lambda examples: pipe.tokenizer(examples[\"output\"]), batched=True)\n",
    "        token_counts = [len(input_ids) for input_ids in tokenized_outputs[\"input_ids\"]]\n",
    "        log.info(f\"Output token counts: min={min(token_counts)}, max={max(token_counts)}\")\n",
    "        generation_kwargs[\"max_new_tokens\"] = ceil(max(token_counts) / 8) * 8\n",
    "\n",
    "    terminators = generation_kwargs.pop(\"terminators\", [])\n",
    "    eos_token_ids = {pipe.tokenizer.eos_token_id}\n",
    "    for terminator in terminators:\n",
    "        if isinstance(terminator, int):\n",
    "            eos_token_ids.add(terminator)\n",
    "        elif isinstance(terminator, str):\n",
    "            eos_token_ids.add(pipe.tokenizer.convert_tokens_to_ids(terminator))\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid terminator token {terminator}.\")\n",
    "    generation_kwargs[\"eos_token_id\"] = sorted(eos_token_ids)\n",
    "\n",
    "    # Generate text\n",
    "    log.info(f\"Running pipeline on dataset with {len(dataset)} samples...\")\n",
    "    generations = flat_pipeline(pipe)(dataset[\"input\"], **generation_kwargs)\n",
    "\n",
    "    # Parse outputs\n",
    "    predictions = [output_parse_fn(text) for text in generations]\n",
    "    references = [output_parse_fn(text) for text in dataset[\"output\"]]\n",
    "\n",
    "    # Create dataframe\n",
    "    dataf = dataset.to_pandas()\n",
    "    dataf[\"generation\"] = generations\n",
    "    dataf[\"prediction\"] = predictions\n",
    "    dataf[\"reference\"] = references\n",
    "\n",
    "    return dataf\n",
    "\n",
    "\n",
    "def evaluate_(\n",
    "    config,\n",
    "    *,\n",
    "    tokenizer=None,\n",
    "    model=None,\n",
    "    metric_kwargs: dict | None = None,\n",
    "    output_parse_fn: Callable[[str], Any] | None = None,\n",
    "):\n",
    "    import evaluate\n",
    "\n",
    "    # Load validation dataset\n",
    "    dataset_config = config.at(\"dataset.validation\")\n",
    "    assert dataset_config, \"Validation dataset is not provided!\"\n",
    "    dataset = load_datasets(dataset_config)\n",
    "    assert len(dataset) > 0, \"Validation dataset is empty!\"\n",
    "    \n",
    "    # Ensure the dataset has input/output columns\n",
    "    cols = dataset[0].keys()\n",
    "    if \"input\" not in cols or \"output\" not in cols:\n",
    "        if \"messages\" not in dataset.column_names:\n",
    "            raise ValueError(\"Dataset must have 'messages' column if 'input' and 'output' columns are not provided.\")\n",
    "        dataset = dataset.map(\n",
    "            lambda x: {\"input\": x[\"messages\"][:-1], \"output\": x[\"messages\"][-1][\"content\"]}\n",
    "        ).remove_columns(\"messages\")\n",
    "\n",
    "    # Prepare text generation pipeline\n",
    "    if tokenizer is None or model is None:\n",
    "        tokenizer, model = _load_tokenizer_model(config)\n",
    "\n",
    "    pipe = make_pipeline(config, tokenizer, model)\n",
    "\n",
    "    dataf = predict(\n",
    "        pipe,\n",
    "        dataset,\n",
    "        output_parse_fn=output_parse_fn,\n",
    "        **config.at(\"evaluation.generation_params\", {}),\n",
    "    )\n",
    "\n",
    "    # Compute metrics\n",
    "    metric = evaluate.load(config.at(\"evaluation.metric\"))\n",
    "    metric_kwargs = metric_kwargs or {}\n",
    "    scores = metric.compute(predictions=dataf[\"prediction\"].values, references=dataf[\"reference\"].values, **metric_kwargs)\n",
    "\n",
    "    return scores, dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
