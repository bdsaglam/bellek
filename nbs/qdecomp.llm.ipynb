{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-hop question decomposition with language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp qdecomp.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import re\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "QUESTION_DECOMPOSITION_SYSTEM_PROMPT_TEMPLATE = \"\"\"Decompose the given question into n sub-questions such that when sub-questions are answered, the original question can be answered correctly.\n",
    "The second subquestion must refer to the answer of the first question by `#1` as in the examples below. Do not create open-ended sub-questions like \"Who is ...\" or \"How is ...\".\n",
    "\n",
    "Avoid statements like 'Here are the decomposed sub-question, ...' or 'Sure! Here are..' or anything along those lines. Just provide sub-questions as illustrated in the examples below.\n",
    "\n",
    "Question: Who founded the journal that published The Review of Communication?\n",
    "Sub-questions:\n",
    "1. Who published The Review of Communication?\n",
    "2. Who founded #1?\n",
    "\n",
    "Question: When was the institute that owned The Collegian founded?\n",
    "Sub-questions:\n",
    "1. Which institute does own The Collegian?\n",
    "2. When #1 founded?\n",
    "\n",
    "Question: {question}\n",
    "Sub-questions:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def parse_llm_generation(output: str):\n",
    "    if output.startswith(\"Sub-questions:\"):\n",
    "        output = output.split(\"Sub-questions:\", 1)[-1]\n",
    "    enumeration_pattern = r'^\\d+\\.\\s'\n",
    "    for line in output.splitlines():\n",
    "        # check if the line starts with enumeration\n",
    "        if not re.match(enumeration_pattern, line):\n",
    "            continue\n",
    "        yield line.split(\".\", 1)[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def make_question_decomposer(\n",
    "    model: str = \"gpt-3.5-turbo\",\n",
    "    client: OpenAI = None,\n",
    "    completion_kwargs: dict | None = None,\n",
    "):\n",
    "    if client is None:\n",
    "        client = OpenAI()\n",
    "\n",
    "    if completion_kwargs is None:\n",
    "        completion_kwargs = {}\n",
    "\n",
    "    def func(question: str) -> list[str]:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": QUESTION_DECOMPOSITION_SYSTEM_PROMPT_TEMPLATE.format(question=question),\n",
    "            },\n",
    "        ]\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **completion_kwargs,\n",
    "        )\n",
    "        text = chat_completion.choices[0].message.content\n",
    "        return list(parse_llm_generation(text))\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In which city was Bovi born?', \"In what county is the Roman Catholic Archdiocese of #1's city located?\"]\n"
     ]
    }
   ],
   "source": [
    "qdecomposer = make_question_decomposer()\n",
    "question = \"What county is the Roman Catholic Archdiocese of the city where Bovi was born located?\"\n",
    "sub_questions = qdecomposer(question=question)\n",
    "print(sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In which city was Bovi born?', 'In which county is the Roman Catholic Archdiocese of #1 located?']\n"
     ]
    }
   ],
   "source": [
    "qdecomposer = make_question_decomposer(model=\"llama-3-70b-tgi\")\n",
    "question = \"What county is the Roman Catholic Archdiocese of the city where Bovi was born located?\"\n",
    "sub_questions = qdecomposer(question=question)\n",
    "print(sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
