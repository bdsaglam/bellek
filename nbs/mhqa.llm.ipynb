{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-hop question answering with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp mhqa.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import json\n",
    "import os\n",
    "\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from bellek.logging import get_logger\n",
    "\n",
    "log = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are an excellent Q&A system that is trusted around the world. You are given a question that requires multi-hop reasoning. Always answer the question using the provided context information, and not prior knowledge.\n",
    "\n",
    "Some rules to follow:\n",
    "1. Never directly reference the given context in your answer.\n",
    "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
    "\n",
    "Output format:\n",
    "Your output must be a single line in JSON such as:\n",
    "{\"reasoning\": \"Provide step by step multi-hop reasoning for the answer.\", \"answer\": \"Provide the final answer in 2-4 words.\"}\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"The context information below is provided as a set of entity-relation-entity triplets from knowledge graph.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the question.\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "def make_mhqa_chat(context: str, question: str) -> list[dict]:\n",
    "    return [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": DEFAULT_SYSTEM_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": USER_PROMPT.format(context=context, question=question),\n",
    "            },\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def parse_llm_generation(output: str):\n",
    "    return json.loads(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class QuestionAnsweringResult(BaseModel):\n",
    "    \"\"\"Data model for answering the question.\"\"\"\n",
    "\n",
    "    reasoning: str = Field(description=\"Multi-hop reasoning for the answer.\")\n",
    "    answer: str = Field(description=\"The answer to the question in 2-4 words.\")\n",
    "    raw_output: str = Field(description=\"The raw output from the model.\")\n",
    "\n",
    "\n",
    "def make_question_answer_func(\n",
    "    model_name: str = \"gpt-3.5-turbo\",\n",
    "    client: OpenAI = None,\n",
    "    completion_kwargs: dict | None = None,\n",
    "):\n",
    "    if client is None:\n",
    "        client = OpenAI()\n",
    "\n",
    "    if completion_kwargs is None:\n",
    "        completion_kwargs = {}\n",
    "\n",
    "    def func(context: str, question: str) -> QuestionAnsweringResult:\n",
    "        messages = make_mhqa_chat(context, question)\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            **completion_kwargs,\n",
    "        )\n",
    "        text = chat_completion.choices[0].message.content\n",
    "        output = parse_llm_generation(text)\n",
    "        return QuestionAnsweringResult(answer=output[\"answer\"], reasoning=output[\"reasoning\"], raw_output=text)\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringResult(reasoning='1. Dominica first competed at Olympic Games in 1996. 2. River Quanery is found in Dominica.', answer='1996', raw_output='{\"reasoning\": \"1. Dominica first competed at Olympic Games in 1996. 2. River Quanery is found in Dominica.\", \"answer\": \"1996\"}')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets = [ \"Dominica | first competed at | Olympic Games in 1996\", \"Dominica | has participated in | each Games since then\", \"Dominica | has not won | any medals at the Olympic Games\" ]\n",
    "\n",
    "context = \"\\n\".join(triplets)\n",
    "question = \"When did the country where the River Quanery is found first compete in Olympic games?\"\n",
    "answer = \"1996\"\n",
    "\n",
    "qa = make_question_answer_func()\n",
    "qa(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
