{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-hop question decomposition with language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp ml.llm.qdecomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "QUESTION_DECOMPOSITION_SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "Decompose the given question into 2 sub-questions such that when sub-questions are answered, the original question can be answered correctly.\n",
    "The second subquestion must refer to the answer of the first question by `#1` as in the examples below. Do not create open-ended sub-questions like \"Who is ...\" or \"How is ...\".\n",
    "\n",
    "Question: What year saw the creation of the region where the county of Hertfordshire is located?\n",
    "Sub-questions:\n",
    "1. In which state is Hertfordshire located?\n",
    "2. When was #1 birthed?\n",
    "\n",
    "Question: When was the institute that owned The Collegian founded?\n",
    "Sub-questions:\n",
    "1. Which institute does own The Collegian?\n",
    "2. When #1 founded?\n",
    "\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def parse_sub_questions(output: str):\n",
    "    flag = False\n",
    "    for line in output.splitlines():\n",
    "        if line.lower().startswith('sub-questions'):\n",
    "            flag = True\n",
    "            continue\n",
    "        if flag:\n",
    "            yield line.split('.', 1)[-1].strip()\n",
    "\n",
    "\n",
    "def make_question_decomposer(llm=None):\n",
    "    if llm is None:\n",
    "        llm = ChatOpenAI(temperature=0, model= 'gpt-3.5-turbo')\n",
    "    system_message_prompt = SystemMessagePromptTemplate.from_template(QUESTION_DECOMPOSITION_SYSTEM_PROMPT_TEMPLATE)\n",
    "    human_message_prompt = HumanMessagePromptTemplate.from_template(\"Question: {question}\")\n",
    "    chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "    chain = LLMChain(llm=llm, prompt=chat_prompt)\n",
    "    def func(question):\n",
    "        out = chain.run(question=question)\n",
    "        return list(parse_sub_questions(out))\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In which town is KNFM licensed?', 'What county is the capital of #1?']\n"
     ]
    }
   ],
   "source": [
    "qdecomposer = make_question_decomposer()\n",
    "question = \"What county is the town where KNFM is licensed the capital of?\"\n",
    "sub_questions = qdecomposer(question=question)\n",
    "print(sub_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
