{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuSiQue evaluation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp musique.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "\n",
    "from bellek.text.utils import fuzzy_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def fuzzy_match_metric(prediction: str, references: list[str]) -> float:\n",
    "    return max([float(fuzzy_match(prediction, ref)) for ref in references])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "musique_metric = evaluate.load(\"bdsaglam/musique\")\n",
    "\n",
    "\n",
    "def compute_scores(predicted_answer: str, reference_answers: list[str]) -> dict:\n",
    "    musique_scores = musique_metric.compute(predictions=[predicted_answer], references=[reference_answers])\n",
    "    fuzzy_match = fuzzy_match_metric(predicted_answer, reference_answers)\n",
    "    return {**musique_scores, \"fuzzy_match\": fuzzy_match}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.0, 'f1': 0.5, 'fuzzy_match': 1.0}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = compute_scores(\"Alexandre the Great\", [\"Alexander the Great\", \"Great Alexander\"])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def calculate_metrics(dataf: pd.DataFrame) -> dict:\n",
    "    prediction_list = dataf[\"predicted_answer\"].tolist()\n",
    "    references_list = dataf[\"answers\"].tolist()\n",
    "    scores_list = [compute_scores(prediction, references) for prediction, references in zip(prediction_list, references_list)]\n",
    "    return pd.DataFrame(scores_list).mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def _exact_match(example):\n",
    "    pred = example['predicted_answer']\n",
    "    return pred is not None and any(pred == ref for ref in example['answers'])\n",
    "\n",
    "def _fuzzy_match(example):\n",
    "    pred = example['predicted_answer']\n",
    "    return pred is not None and any((pred in ref) or (ref in pred) or fuzzy_match(pred, ref) for ref in example['answers'])\n",
    "\n",
    "\n",
    "def compare_answers(dataf: pd.DataFrame) -> pd.DataFrame:\n",
    "    dataf['exact_match'] = dataf.apply(_exact_match, axis=1)\n",
    "    dataf['fuzzy_match'] = dataf.apply(_fuzzy_match, axis=1)\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
