{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuSiQue evaluation utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp musique.eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "import collections\n",
    "import re\n",
    "import string\n",
    "from typing import Callable\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from bellek.text.utils import fuzzy_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def fuzzy_match_metric(prediction: str, references: list[str]) -> float:\n",
    "    return max([float(fuzzy_match(prediction, ref)) for ref in references])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text: str) -> str:\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text: str) -> str:\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text: str) -> str:\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text: str) -> str:\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s: str) -> list[str]:\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "\n",
    "def compute_exact_match(a_gold: str, a_pred: str) -> int:\n",
    "    \"\"\"Compute the Exact Match (EM) score between a gold answer and a prediction.\"\"\"\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1(a_gold: str, a_pred: str) -> float:\n",
    "    \"\"\"Compute the F1 score between a gold answer and a prediction.\"\"\"\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(\n",
    "    metric_fn: Callable[[str, str], float],\n",
    "    prediction: str,\n",
    "    ground_truths: list[str],\n",
    ") -> float:\n",
    "    \"\"\"Calculate the maximum metric score for a prediction over all ground truths.\"\"\"\n",
    "    scores_for_ground_truths = [metric_fn(prediction, ground_truth) for ground_truth in ground_truths]\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def compute_scores(prediction: str, reference: list[str]) -> dict:\n",
    "    exact_match = metric_max_over_ground_truths(compute_exact_match, prediction, reference)\n",
    "    f1 = metric_max_over_ground_truths(compute_f1, prediction, reference)\n",
    "    fuzzy_match = fuzzy_match_metric(prediction, reference)\n",
    "    return {\"exact_match\": exact_match, \"f1\": f1, \"fuzzy_match\": fuzzy_match}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0, 'f1': 0.5, 'fuzzy_match': 1.0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = compute_scores(\"Alexandre the Great\", [\"Alexander the Great\", \"Great Alexander\"])\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def calculate_metrics(dataf: pd.DataFrame) -> dict:\n",
    "    prediction_list = dataf[\"predicted_answer\"].tolist()\n",
    "    references_list = dataf[\"answers\"].tolist()\n",
    "    scores_list = [compute_scores(prediction, references) for prediction, references in zip(prediction_list, references_list)]\n",
    "    return pd.DataFrame(scores_list).mean().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "def _exact_match(example):\n",
    "    pred = example['predicted_answer']\n",
    "    return pred is not None and any(pred == ref for ref in example['answers'])\n",
    "\n",
    "def _fuzzy_match(example):\n",
    "    pred = example['predicted_answer']\n",
    "    return pred is not None and any((pred in ref) or (ref in pred) or fuzzy_match(pred, ref) for ref in example['answers'])\n",
    "\n",
    "\n",
    "def compare_answers(dataf: pd.DataFrame) -> pd.DataFrame:\n",
    "    dataf['exact_match'] = dataf.apply(_exact_match, axis=1)\n",
    "    dataf['fuzzy_match'] = dataf.apply(_fuzzy_match, axis=1)\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
