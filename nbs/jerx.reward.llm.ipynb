{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward model JERX task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp jerx.reward.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import os\n",
    "import instructor\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI, BadRequestError\n",
    "\n",
    "from bellek.text.utils import fuzzy_match\n",
    "from bellek.logging import get_logger\n",
    "\n",
    "log = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|hide\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = \"\"\"You are an expert Q&A system that is trusted around the world. You are given a question that requires multi-hop reasoning. Always answer the question using the provided context information, and not prior knowledge.\n",
    "Some rules to follow:\n",
    "1. Never directly reference the given context in your answer.\n",
    "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT = \"\"\"The context information below is provided as a set of entity-relation-entity triplets from knowledge graph.\n",
    "---------------------\n",
    "{context}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the question.\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "class QuestionAnsweringResult(BaseModel):\n",
    "    \"\"\"Data model for answering the question.\"\"\"\n",
    "\n",
    "    answer: str = Field(description=\"The answer to the question in 2-4 words.\")\n",
    "    reasoning: str = Field(description=\"Multi-hop reasoning for the answer.\")\n",
    "\n",
    "\n",
    "def make_question_answer_func(model_name: str = \"gpt-3.5-turbo\", client: OpenAI = None):\n",
    "    if client is None:\n",
    "        client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), base_url=os.getenv(\"OPENAI_API_BASE\"))\n",
    "    client = instructor.from_openai(client)\n",
    "\n",
    "    def func(context: str, question: str) -> QuestionAnsweringResult:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": DEFAULT_SYSTEM_PROMPT,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": USER_PROMPT.format(context=context, question=question),\n",
    "            },\n",
    "        ]\n",
    "        return client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            response_model=QuestionAnsweringResult,\n",
    "            messages=messages,\n",
    "        )\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "class RewardAssessment(QuestionAnsweringResult):\n",
    "    reward: float = Field(description=\"The reward value for the answer.\")\n",
    "\n",
    "def make_reward_func(model_name: str = \"gpt-3.5-turbo\", answer_comparator=fuzzy_match):\n",
    "    qa = make_question_answer_func(model_name)\n",
    "\n",
    "    def reward(context: str, question: str, answers: list[str]) -> RewardAssessment:\n",
    "        try:\n",
    "            qa_result = qa(context, question)\n",
    "        except BadRequestError as e:\n",
    "            log.warning(f\"Failed to assess generation: {e}\")\n",
    "            return RewardAssessment(answer=\"\", reasoning=str(e), reward=0.0)\n",
    "        correct = any(answer_comparator(qa_result.answer, answer) for answer in answers)\n",
    "        reward = 1.0 if correct else 0.0\n",
    "        return RewardAssessment(**qa_result.dict(), reward=reward)\n",
    "\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringResult(answer='1996', reasoning='The country where the River Quanery is found first competed in Olympic Games in 1996, as mentioned in the context.')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triplets = [ \"Dominica | first competed at | Olympic Games in 1996\", \"Dominica | has participated in | each Games since then\", \"Dominica | has not won | any medals at the Olympic Games\" ]\n",
    "\n",
    "context = \"\\n\".join(triplets)\n",
    "question = \"When did the country where the River Quanery  is found first compete in Olympic games?\"\n",
    "answer = \"1996\"\n",
    "\n",
    "qa = make_question_answer_func()\n",
    "qa(context=context, question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward_func = make_reward_func(\"llama3-70b-8192\")\n",
    "# assert reward_func(context, question, [answer]).reward == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_func = make_reward_func(\"gpt-4-turbo\")\n",
    "assert reward_func(context, question, [answer]).reward == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
