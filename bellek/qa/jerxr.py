# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/qa.jerxr.ipynb.

# %% auto 0
__all__ = ['log', 'DEFAULT_SYSTEM_PROMPT', 'USER_PROMPT', 'make_qa_chat', 'parse_llm_generation', 'QuestionAnsweringResult',
           'make_question_answer_func']

# %% ../../nbs/qa.jerxr.ipynb 4
import json
import os

from openai import OpenAI
from pydantic import BaseModel, Field

from ..logging import get_logger

log = get_logger(__name__)

# %% ../../nbs/qa.jerxr.ipynb 5
DEFAULT_SYSTEM_PROMPT = """You are an excellent question-answering system that is trusted around the world. You base your answers solely on the context information provided and not on prior knowledge. You must provide correct step-by-step reasoning for your answers. You first extract related entity-relation-entity triplets from the context and then answer the question. For instance, 

Context: "Glenhis Hernández (born 7 October 1990 in Havana) is a taekwondo practitioner from Cuba. She was the 2013 World
Champion in middleweight.

The current mayor of Havana ("President of the People's Power Provincial Assembly") is Marta Hernández Romero, she
was elected on March 5, 2011."
Question: "Who is the current mayor of Havana?"
Triplets: "Glenhis Hernández (Athlete) | born on | October 7, 1990
Glenhis Hernández (Athlete) | birth place | Havana
Glenhis Hernández (Athlete) | specializes in | taekwondo
Glenhis Hernández (Athlete) | won | 2013 World Champion title (Middleweight)
Marta Hernández Romero (Politician) | serves as | mayor of Havana
Marta Hernández Romero (Politician) | holds | the position of "President of the People's Power Provincial Assembly"
Marta Hernández Romero (Politician) | elected on | March 5, 2011."

Some rules to follow:
1. Never directly reference the given context in your answer.
2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.

Output format:
Your output must be a single line in JSON such as:
{"triplets": "A string listing triplets extracted from the context", "reasoning": "Provide step by step reasoning for the answer.", "answer": "Provide the final answer in 2-4 words."}
"""

USER_PROMPT = """The context information below is provided.
---------------------
{context}
---------------------
Given the context information and not prior knowledge, answer the question.
{question}
"""

def make_qa_chat(context: str, question: str) -> list[dict]:
    return [
            {
                "role": "system",
                "content": DEFAULT_SYSTEM_PROMPT,
            },
            {
                "role": "user",
                "content": USER_PROMPT.format(context=context, question=question),
            },
        ]

# %% ../../nbs/qa.jerxr.ipynb 6
def parse_llm_generation(output: str):
    return json.loads(output)

# %% ../../nbs/qa.jerxr.ipynb 7
class QuestionAnsweringResult(BaseModel):
    """Data model for answering the question."""

    triplets: str = Field(description="The entity-relation-entity triplets extracted from the context.")
    reasoning: str = Field(description="Concise reasoning for the answer.")
    answer: str = Field(description="The answer to the question in 2-4 words.")
    raw_output: str = Field(description="The raw output from the model.")


def make_question_answer_func(
    model_name: str = "gpt-3.5-turbo",
    client: OpenAI = None,
    completion_kwargs: dict | None = None,
):
    if client is None:
        client = OpenAI()

    if completion_kwargs is None:
        completion_kwargs = {}

    def func(context: str, question: str) -> QuestionAnsweringResult:
        messages = make_qa_chat(context, question)
        chat_completion = client.chat.completions.create(
            model=model_name,
            messages=messages,
            **completion_kwargs,
        )
        text = chat_completion.choices[0].message.content
        output = parse_llm_generation(text)
        return QuestionAnsweringResult(answer=output["answer"], triplets=output['triplets'], reasoning=output["reasoning"], raw_output=text)

    return func
